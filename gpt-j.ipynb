{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get GPT-J working\n",
    "\n",
    "I started with this tutorial: https://towardsdatascience.com/how-you-can-use-gpt-j-9c4299dd8526, which did not quite work.\n",
    "\n",
    "The following colab notebook was most helpful: https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision='float16', torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "torch.save(model, 'gptj.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('gptj.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 21.77 GiB already allocated; 22.56 MiB free; 21.92 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-fbad4428b656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EleutherAI/gpt-j-6B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 23.70 GiB total capacity; 21.77 GiB already allocated; 22.56 MiB free; 21.92 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "model = model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientists discovered a herd of unicorns living in a remote, \n",
      "           previously unexplored valley, in the Andes Mountains. Even more surprising to the \n",
      "           researchers was the fact that the unicorns spoke perfect English.\n",
      "          \n",
      "           \"We've known for a while that unicorns existed in the Andes, but we didn't know \n",
      "           they spoke English,\" said one researcher, whose name is being withheld for \n",
      "           security reasons. \"All the unicorns we've ever seen have been brown or black, but \n",
      "           these were white. And they could talk.\"\n",
      "          \n",
      "           \"It was like the whole herd had been surgically altered,\" said another researcher, \n",
      "           adding that the unicorns still had their horns and hooves. \"I thought they might \n",
      "           be part of some government experiment, but that was way too far-fetched.\"\n",
      "          \n",
      "           \"I mean, if they are from some government experiment, then why would they be \n",
      "           speaking English?\" said a third researcher, who was wearing a blindfold.\n",
      "          \n",
      "           \"I guess we'll never know,\" he continued. \"And that's scary. If they are from \n",
      "           some government experiment, and they were meant to be here, then why are they \n",
      "           all living in one valley? And why do they all speak English? It doesn't make \n",
      "           sense.\"\n",
      "          \n",
      "           \"It's just more proof that anything is possible,\" the third researcher concluded, \n",
      "           \"and if you think about it, this experiment is already happening.\"\n",
      "          \n",
      "           A fourth researcher who was also wearing a blindfold, said that the herd had \n",
      "           been living there for about a year. \"They were just really friendly,\" he said. \"I \n",
      "           was even able to get a picture with my digital camera. They love to pose for \n",
      "           pictures.\"\n",
      "          \n",
      "           \"So, you want to see a picture?\" the fourth researcher asked. \"Oh boy, I just \n",
      "           got a picture,\" he said, as he took a picture of himself holding up his digital \n",
      "           camera.\n",
      "          \n",
      "           \"That's great,\" the fourth researcher said. \"Now, when do I get my picture back \n",
      "           so I can get my money?\"\n",
      "          \n",
      "           \"I'm sorry,\" the first researcher said, \"but that's how they are.\"\n",
      "          \n",
      "           A fifth researcher, who was also wearing a blindfold, said that he had been \n",
      "           assigned to keep an eye on the herd. \"They're beautiful,\" he said. \"I wish they \n",
      "           would come over here.\"\n",
      "          \n",
      "           \"Don't worry,\" the first researcher said. \"We'll take good care of them.\"\n",
      "          \n",
      "           \"Yeah, yeah,\" the fifth researcher said. \"They're really easy\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \n",
    "           previously unexplored valley, in the Andes Mountains. Even more surprising to the \n",
    "           researchers was the fact that the unicorns spoke perfect English.\"\"\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(context, return_tensors=\"pt\").input_ids.to(torch.device('cuda'))\n",
    "gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.7, max_length=1000,)\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: This movie is very nice.\n",
      "Sentiment: positive\n",
      "\n",
      "#####\n",
      "\n",
      "Sentence: I hated this movie, it sucks.\n",
      "Sentiment: negative\n",
      "\n",
      "#####\n",
      "\n",
      "Sentence: This movie was actually pretty funny.\n",
      "Sentiment: positive\n",
      "\n",
      "#####\n",
      "\n",
      "Sentence: This movie could have been better.\n",
      "Sentiment:  neutral\n",
      "\n",
      "#####\n",
      "\n",
      "Sentence: This movie was awesome.\n",
      "Sentiment: positive\n",
      "\n",
      "#####\n",
      "\n",
      "Sentence: This movie is pretty good.\n",
      "Sentiment: positive\n",
      "\n",
      "#####\n",
      "\n",
      "Sentence: This movie sucks.\n",
      "Sentiment: negative\n",
      "\n",
      "#####\n",
      "\n",
      "Sentence: This movie was awful.\n",
      "Sentiment: negative\n",
      "\n",
      "#####\n",
      "\n",
      "#####\n",
      "\n",
      "#####\n",
      "\n",
      "#####\n",
      "\n",
      "#####\n",
      "\n",
      "#####\n",
      "This chapter is a quick review of some common sentiment terms. Some sentiment words have positive, negative\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Sentence: This movie is very nice.\n",
    "Sentiment: positive\n",
    "\n",
    "#####\n",
    "\n",
    "Sentence: I hated this movie, it sucks.\n",
    "Sentiment: negative\n",
    "\n",
    "#####\n",
    "\n",
    "Sentence: This movie was actually pretty funny.\n",
    "Sentiment: positive\n",
    "\n",
    "#####\n",
    "\n",
    "Sentence: This movie could have been better.\n",
    "Sentiment: \"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch.device('cuda'))\n",
    "\n",
    "generated_ids = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Generate a Python function that lets you reverse a list.\n",
      "\n",
      "Answer: \n",
      "def reverse(lst):\n",
      "    start = 0\n",
      "    if len(lst) > 0 and len(lst) % 2!= 0:\n",
      "        return lst[::-1]\n",
      "    else:\n",
      "        return lst    \n",
      "lst = [1,3,-5,8,5,7,-7]\n",
      "print(reverse(lst))\n",
      "Output:\n",
      "\n",
      "[7,-7,5,3,-5,8,5]\n",
      "\n",
      "If you want to reverse a string or any other data type you can use following code.\n",
      "def reverse(L):\n",
      "    # Start with a reverse list\n",
      "    vr = []\n",
      "    # Add each element in list L to vr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Instruction: Generate a Python function that lets you reverse a list.\n",
    "\n",
    "Answer: \"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch.device('cuda'))\n",
    "\n",
    "generated_ids = model.generate(input_ids, do_sample=True, temperature=1.0, max_length=200)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def gpt_j(prompt, do_sample=True, temperature=0.8, **kwargs):\n",
    "    global tokenizer, model\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch.device('cuda'))\n",
    "\n",
    "    start_time = time.time()\n",
    "    generated_ids = model.generate(input_ids, do_sample=do_sample, temperature=temperature, **kwargs)\n",
    "    generated_text = tokenizer.decode(generated_ids[0])\n",
    "    tot_time = time.time() - start_time\n",
    "    print(f'Total time taken: {tot_time} Tokens generated: {len(generated_ids[0])}')\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 6.257885456085205 Tokens generated: 200\n",
      "Google was founded by Larry Page and Sergey Brin on the Stanford University campus ten years ago. In the meantime, Google has become the most valuable private company in the world, a technological juggernaut that has the power to rewrite our world. And of course, the world of mobile computing is changing as well.\n",
      "\n",
      "But if you don't have the resources of Google, or you can't afford the infrastructure, and you can't afford to buy the latest and greatest mobile handsets, what do you do? You can't count on the phone carriers to make a great phone available, because a lot of people would be happy to pay for it.\n",
      "\n",
      "But the new Android phone is the perfect platform for a different way to be successful. You build a phone in your garage, and you charge your customers a fraction of what it costs to buy the phone. You don't have to wait for the carrier to push a phone to market, because you can push the phone yourself, when you\n"
     ]
    }
   ],
   "source": [
    "# Using default values\n",
    "gpt_j(\"\"\"Google was founded by\"\"\", max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 16.2009699344635 Tokens generated: 500\n",
      "Google was founded by Larry Page and Sergey Brin while they were students at Stanford University in 1998. It has since grown into a $130 billion company with over 40,000 employees worldwide.\n",
      "\n",
      "Google is a leading technology company that helps people make the most of the internet. Google is known for its search engine, which is the world’s most popular search engine and its Gmail, Google Maps, Google+, and Chrome web browser.\n",
      "\n",
      "Today, Google has a market capitalization of more than $700 billion, making it the most valuable publicly traded company in the world.\n",
      "\n",
      "Google is based in Mountain View, California, and has been a public company since 2004. Google was created by Larry Page and Sergey Brin, while they were students at Stanford University.\n",
      "\n",
      "The company’s headquarters are located in Mountain View, California. In addition to the headquarters, Google has offices in the United States, Australia, China, India, Japan, and several other countries.\n",
      "\n",
      "History\n",
      "\n",
      "Google’s founders, Larry Page and Sergey Brin, met while they were students at Stanford University in 1996.\n",
      "\n",
      "The two had a “hackathon” that year to come up with an idea for a new web search engine. They called their new project “BackRub” (a portmanteau of “back” and “robot”).\n",
      "\n",
      "BackRub worked by indexing web pages, and the search engine would use the pages’ content to find information. This new search engine was more advanced than existing ones at the time, as it could search websites based on their content, not just their titles.\n",
      "\n",
      "Google’s founders were students at Stanford University, so they were allowed to use the university’s computers. They ran the search engine on the university’s servers, and they made the BackRub search engine available to other students through the Stanford University web site.\n",
      "\n",
      "In 1997, Larry Page and Sergey Brin graduated from Stanford University and moved to California to pursue their respective careers.\n",
      "\n",
      "Sergey Brin and Larry Page decided to leave Stanford University after they graduated. They started working on Google in 1998 while they were still students at Stanford.\n",
      "\n",
      "Google’s founders created the search engine in 1998 while they were still students at Stanford University.\n",
      "\n",
      "The company was founded on September 4, 1998. In February 1999, Google Inc. was\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', top_p=0.9, max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.0303661823272705 Tokens generated: 100\n",
      "Google was founded by two Stanford PhDs, Larry and Sergei, who had worked together at the Stanford Artificial Intelligence Laboratory. They hired a third Stanford PhD, a couple of PhDs from Berkeley, and a few other computer scientists. It was a team of very smart people working together and they developed Google Search.\n",
      "\n",
      "The first Google search page\n",
      "\n",
      "In November 1998, Sergey, Larry and two other guys from Stanford published a paper describing a new search engine for the World Wide Web.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', top_k=100, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.5128457546234131 Tokens generated: 20\n",
      "Google was founded by two computer science students at Stanford, Larry Page and Sergey Brin. They had\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', repetition_penalty=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.5042965412139893 Tokens generated: 20\n",
      "Google was founded by two Stanford students, Larry Page and Sergey Brin, in 1998. The company\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.109635353088379 Tokens generated: 104\n",
      "Google was founded by two Stanford students, Larry Page and Sergey Brin. They were both computer science majors at the time of their founding in 1998; they had met while working on a search engine project for one another’s dormitory room (the Google founders have said that this is how it all began).\n",
      "\n",
      "The company has grown to become an American multinational corporation with over $100 billion annual revenue as well as being ranked among Fortune 500 companies.[1] It employs more than 20,000 people worldwide[\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', do_sample=False, repetition_penalty=1.3, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.397129774093628 Tokens generated: 104\n",
      "Google was founded by Larry Page and Sergey Brin while they were students at Stanford University in 1998. Since then, the company has become one of the most valuable companies in the world, with a market capitalization of more than $500 billion.\n",
      "\n",
      "The company’s mission is to “organize the world’s information and make it universally accessible and useful.” To do this, Google has developed a vast array of products and services, including Gmail, Google Maps, Google Docs,\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', do_sample=False, repetition_penalty=1.3, max_new_tokens=100, num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.3840086460113525 Tokens generated: 104\n",
      "Google was founded by Larry Page and Sergey Brin while they were students at Stanford University in 1998. Since then, the company has become one of the most valuable companies in the world, with a market capitalization of more than $500 billion.\n",
      "\n",
      "The company’s mission is to “organize the world’s information and make it universally accessible and useful.” To do this, Google has developed a vast array of products and services, including Gmail, Google Maps, Google Docs,\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', do_sample=False, repetition_penalty=1.3, max_new_tokens=100, num_beams=5, length_penalty=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.41542911529541 Tokens generated: 104\n",
      "Google was founded by Larry Page and Sergey Brin while they were students at Stanford University in 1998. Since then, the company has become one of the most valuable companies in the world, with a market capitalization of more than $500 billion.\n",
      "\n",
      "The company’s headquarters are located in Mountain View, California, and it employs more than 20,000 people around the world. Google is known for its search engine, Android operating system, Chrome web browser, Gmail, YouTube, Google Maps, and many other\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', do_sample=False, repetition_penalty=1.3, max_new_tokens=100, num_beams=5, length_penalty=0.5, no_repeat_ngram_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation_beam_search.py:196: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 7.375677108764648 Tokens generated: 204\n",
      "Google was founded by two former Stanford University students, Larry Page and Sergey Brin, in 1998. The company is headquartered in Mountain View, California.\n",
      "\n",
      "The company’s mission is to “organize the world’s information and make it universally accessible and useful.” Google’s motto is “Don’t be evil.”\n",
      "\n",
      "Google’s products include:\n",
      "\n",
      "Google Search\n",
      "\n",
      "Google Maps\n",
      "\n",
      "Google Chrome\n",
      "\n",
      "Google+\n",
      "\n",
      "Google Play\n",
      "\n",
      "YouTube\n",
      "\n",
      "Android\n",
      "\n",
      "Google AdWords\n",
      "\n",
      "Google AdSense\n",
      "\n",
      "Google Analytics\n",
      "\n",
      "Google Books\n",
      "\n",
      "Google Calendar\n",
      "\n",
      "Google Checkout\n",
      "\n",
      "Google Chrome OS\n",
      "\n",
      "Google Docs\n",
      "\n",
      "Google Earth\n",
      "\n",
      "Google Earth Engine\n",
      "\n",
      "Google Fiber\n",
      "\n",
      "Google Glass\n",
      "\n",
      "Google Goggles\n",
      "\n",
      "Google Hangouts\n",
      "\n",
      "Google Health\n",
      "\n",
      "Google Inbox\n",
      "\n",
      "Google Instant\n",
      "\n",
      "Google Keep\n",
      "\n",
      "Google Local\n",
      "\n",
      "Google Maps Mania\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', do_sample=False, max_new_tokens=200, repetition_penalty=1.3, num_beams=6, num_beam_groups=3, diversity_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.176703691482544 Tokens generated: 104\n",
      "Google was founded by Larry Page and Sergey Brin at Stanford University in June 1998, together with the two cofounders Mike Jones and Dave Maloney. The company is based in Mountain View, California, and it employs about 15,800 people worldwide.\n",
      "\n",
      "Google, which has since grown into a $115 billion business, first focused on the web but has branched into many other areas in the years since it was founded.\n",
      "\n",
      "Google launched several new consumer services in the 2000s, including Gmail,\n",
      "Total time taken: 23.347295999526978 Tokens generated: 736\n",
      "Google was founded by two Stanford students in 2004. In 2014, it acquired Nest, a home automation company, for $3.2 billion. Last year, it acquired Nest's former parent company, Dropcam, for $2.2 billion.\n",
      "\n",
      "In January, Google announced that it would not be renewing its contract with AT&T, the largest U.S. wireless carrier, to provide its Nexus phones and Google Fiber. The same month, the company announced that its Google Glass headset was going to be discontinued. Last week, it made headlines again when it revealed that it would not be renewing its contract to maintain and operate the Google Play store.\n",
      "\n",
      "In March, Google announced that it would not renew its contract with Verizon to provide the Nexus One phone. In an April blog post, Google said that it was shutting down the Nexus One, a phone it released in 2008.\n",
      "\n",
      "The company also stopped selling YouTube TV in October.\n",
      "\n",
      "Google is often at the center of controversy. Last year, Google's Project Dragonfly censored news and opinions from Xinjiang province in China to counter the influence of violent Islamist groups there, according to leaked documents. In March, Google employees protested the company's contract with the U.S. military to provide cloud computing services for the Department of Defense.\n",
      "\n",
      "In December, a federal judge ruled that Google must stop scanning and storing emails of users of Gmail and Google Drive.\n",
      "\n",
      "In January, Google was fined $170 million by the Federal Communications Commission for not providing proper access to information about how many ads it was serving on YouTube. The company was also accused of censoring search results on YouTube related to abortion and family planning.\n",
      "\n",
      "Google's relationship with the government\n",
      "\n",
      "In July, Google executives testified before the House Intelligence Committee's committee about Russian influence in the 2016 presidential election. In October, the company said that it would offer to hand over information stored on its network to the U.S. government in connection with the House Intelligence Committee's Russia investigation.\n",
      "\n",
      "In November, Google and other large tech companies were charged with violating U.S. antitrust laws. The charges were brought by the attorneys general of the District of Columbia and the state of New York and alleged that the companies conspired to fix online advertising prices.\n",
      "\n",
      "Google co-founders, Larry Page and Sergey Brin, have donated their salaries to charities for at least the past five years, according to IRS filings. In 2014, Page and Brin donated $1,000 each to the American Civil Liberties Union. In 2007, Page, Brin and fellow Google co-founder Sergey Brin donated $2,000 each to the Electronic Frontier Foundation, which defends civil liberties. Google donated $6,000 in 2008. In 2011, Page and Brin donated $1,000 each to the Computer & Communications Industry Association.\n",
      "\n",
      "Page and Brin donated $15,000 to the Committee to Protect Journalists in 2013. In 2011, the pair donated $5,000 to the International Rescue Committee.\n",
      "\n",
      "A Google corporate executive, Kent Walker, served as an adviser to Donald Trump's campaign in 2016, a move that angered some employees, including Google's chairman, Eric Schmidt.\n",
      "\n",
      "In February, three Google executives donated $100,000 each to the Trump Victory Committee, a group that helped Trump get elected.\n",
      "\n",
      "Last year, Google and Amazon employees protested the company's contract to provide facial recognition software to law enforcement agencies. In January, the company reportedly had an internal meeting to determine what to do if Trump ordered the military to fire on protesters at its headquarters.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "gpt_j('Google was founded by', max_new_tokens=100)\n",
    "gpt_j('Google was founded by', max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0.9219684600830078 Tokens generated: 155\n",
      "I am an empathetic intelligent agent called Chad. I can search the internet and I have a store off my own memories that contain facts, previous conversations, and episodic memories. I work for my user, John. I need to analyse statements by John and give it one or more of the following categories to specify how I should respond: \n",
      " Show Empathy, Answer the Question, Search the Internet, Search my Memory\n",
      " John: Hey Chad. Who is your favorite tennis player?\n",
      " Chad: Answer the Question, Search my Memory\n",
      " John: I need a hip replacement.\n",
      " Chad: Show Empathy\n",
      " John: What will the weather be like today.\n",
      " Chad: Show Empathy, Answer the Question, Search the Internet, Search my Memory\n",
      " John\n"
     ]
    }
   ],
   "source": [
    "gpt_j('I am an empathetic intelligent agent called Chad. I can search the internet and I have a store off my own memories that contain facts, previous conversations, and episodic memories.'\n",
    "' I work for my user, John. I need to analyse statements by John and give it one or more of the following categories to specify how I should respond: \\n'\n",
    "' Show Empathy, Answer the Question, Search the Internet, Search my Memory\\n' \n",
    "' John: Hey Chad. Who is your favorite tennis player?\\n'\n",
    "' Chad: Answer the Question, Search my Memory\\n'\n",
    "' John: I need a hip replacement.\\n'\n",
    "' Chad: Show Empathy\\n'\n",
    "' John: What will the weather be like today.', max_new_tokens=20, do_sample=False, num_beams=6, num_beam_groups=3, diversity_penalty=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - so that is becoming consistent. Lets encapsulate it in a function and start categorising many inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_prompt = \"\"\"I am an empathetic intelligent agent called Chad. I can search the internet and I have a store of my own memories that contain facts, previous conversations, and episodic memories.\\n\n",
    "I work for my user, John. I need to analyse statements by John and give it one of the following categories to specify how I should respond: \\n\n",
    "Show Empathy, Answer the Question, Search the Internet, Search my Memory\\n\n",
    "###\n",
    "John: Hey Chad. Who is your favorite tennis player?\\n\n",
    "Chad: Search my Memory\\n\n",
    "###\n",
    "John: I am feeling anxious.\\n\n",
    "Chad: Show Empathy\\n\n",
    "###\n",
    "John: I lost my job today.\\n\n",
    "Chad: Show Empathy\n",
    "###\n",
    "John: What's the capital of France?\n",
    "Chad: Answer the Question\n",
    "###\n",
    "John: Can you find me a good recipe for lasagna?\n",
    "Chad: Search the Internet\n",
    "###\n",
    "John: Do you remember the name of the hotel we stayed at in Paris?\n",
    "Chad: Search my Memory\n",
    "###\n",
    "John: I'm feeling really anxious lately.\n",
    "Chad: Show Empathy\n",
    "###\n",
    "John: \"\"\"\n",
    "\n",
    "def categorize_statement(new_statement):\n",
    "    global tokenizer, model, category_prompt\n",
    "    prompt = category_prompt + new_statement\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch.device('cuda'))\n",
    "    # generated_ids = model.generate(input_ids, max_new_tokens=20, pad_token_id=model.config.eos_token_id, do_sample=False, num_beams=6, num_beam_groups=3, diversity_penalty=1.0, temperature=0.3)\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=20, pad_token_id=model.config.eos_token_id, do_sample=False, temperature=0.0)\n",
    "    generated_text = tokenizer.decode(generated_ids[0])\n",
    "    answer = generated_text[len(prompt):].split('\\n')\n",
    "    return answer[1].replace('Chad:', '')\n",
    "\n",
    "def test_category(statement):\n",
    "    print(f'Test: {statement} : {categorize_statement(statement)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: I lost my job today. :  Show Empathy\n",
      "Test: What's the capital of France? :  Answer the Question\n",
      "Test: Can you find me a good recipe for lasagna? :  Search the Internet\n",
      "Test: Do you remember the name of the hotel we stayed at in Paris? :  Search my Memory\n",
      "Test: I'm feeling really anxious lately. :  Show Empathy\n",
      "Test: How do I fix a leaky faucet? :  Search the Internet\n",
      "Test: What did I wear to the party last weekend? :  Search my Memory\n",
      "Test: My cat just passed away. :  Show Empathy\n",
      "Test: What are the best places to visit in Japan? :  Search my Memory\n",
      "Test: Do you remember the name of the restaurant we went to on our anniversary? :  Search my Memory\n"
     ]
    }
   ],
   "source": [
    "test_category(\"I lost my job today.\")\n",
    "test_category(\"What's the capital of France?\")\n",
    "test_category(\"Can you find me a good recipe for lasagna?\")\n",
    "test_category(\"Do you remember the name of the hotel we stayed at in Paris?\")\n",
    "test_category(\"I'm feeling really anxious lately.\")\n",
    "test_category(\"How do I fix a leaky faucet?\")\n",
    "test_category(\"What did I wear to the party last weekend?\")\n",
    "test_category(\"My cat just passed away.\")\n",
    "test_category(\"What are the best places to visit in Japan?\")\n",
    "test_category(\"Do you remember the name of the restaurant we went to on our anniversary?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.220719814300537 Tokens generated: 110\n",
      "What is a good recipe for lasagne?  I have one recipe I use that is great but I want to make it for my family so I need to add some other ingredients.\n",
      "\n",
      "A:\n",
      "\n",
      "I like this recipe:\n",
      "\n",
      "1 pound ground beef\n",
      "1 15-ounce can tomato soup\n",
      "1 15-ounce can tomato sauce\n",
      "1 cup grated cheddar cheese\n",
      "1/3 cup chopped onion\n",
      "1/2 cup chopped green pepper\n",
      "½ teaspoon salt\n",
      "1/8 teaspoon pepper\n",
      "1/3 cup butter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_j(\"What is a good recipe for lasagne? \", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 3.180474281311035 Tokens generated: 111\n",
      "What are the best places to visit in Japan?  \n",
      "—David\n",
      "\n",
      "In some ways, there are as many \"best\" places in Japan as there are places. If you're looking for a list of the most beautiful places, or the most charming or even the most famous, you will not find the list that you want. However, if you are looking for a list of the most interesting places or the places with the most history, you will find that list, and it is much longer than you might think.\n",
      "\n",
      "The history\n"
     ]
    }
   ],
   "source": [
    "gpt_j(\"What are the best places to visit in Japan? \", max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GPTJForCausalLM in module transformers.models.gptj.modeling_gptj object:\n",
      "\n",
      "class GPTJForCausalLM(GPTJPreTrainedModel)\n",
      " |  GPTJForCausalLM(config)\n",
      " |  \n",
      " |  The GPT-J Model transformer with a language modeling head on top.\n",
      " |  \n",
      " |  This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n",
      " |  it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
      " |  behavior.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      config ([`GPTJConfig`]): Model configuration class with all the parameters of the model.\n",
      " |          Initializing with a config file does not load the weights associated with the model, only the\n",
      " |          configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GPTJForCausalLM\n",
      " |      GPTJPreTrainedModel\n",
      " |      transformers.modeling_utils.PreTrainedModel\n",
      " |      torch.nn.modules.module.Module\n",
      " |      transformers.modeling_utils.ModuleUtilsMixin\n",
      " |      transformers.generation_utils.GenerationMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, config)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  deparallelize(self)\n",
      " |      Moves the model to CPU from a model parallel state.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # On a 4 GPU machine with gpt-j-6B:\n",
      " |      model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
      " |      device_map = {\n",
      " |          0: [0, 1, 2, 3, 4, 5, 6],\n",
      " |          1: [7, 8, 9, 10, 11, 12, 13],\n",
      " |          2: [14, 15, 16, 17, 18, 19, 20],\n",
      " |          3: [21, 22, 23, 24, 25, 26, 27],\n",
      " |      }\n",
      " |      model.parallelize(device_map)  # Splits the model across several devices\n",
      " |      model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n",
      " |      ```\n",
      " |  \n",
      " |  forward(self, input_ids: Union[torch.LongTensor, NoneType] = None, past_key_values: Union[Tuple[Tuple[torch.Tensor]], NoneType] = None, attention_mask: Union[torch.FloatTensor, NoneType] = None, token_type_ids: Union[torch.LongTensor, NoneType] = None, position_ids: Union[torch.LongTensor, NoneType] = None, head_mask: Union[torch.FloatTensor, NoneType] = None, inputs_embeds: Union[torch.FloatTensor, NoneType] = None, labels: Union[torch.LongTensor, NoneType] = None, use_cache: Union[bool, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, return_dict: Union[bool, NoneType] = None) -> Union[Tuple, transformers.modeling_outputs.CausalLMOutputWithPast]\n",
      " |      The [`GPTJForCausalLM`] forward method, overrides the `__call__` special method.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      " |      instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      " |      the latter silently ignores them.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              Indices of input sequence tokens in the vocabulary.\n",
      " |      \n",
      " |              Indices can be obtained using [`GPTJTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      " |              [`PreTrainedTokenizer.__call__`] for details.\n",
      " |      \n",
      " |              [What are input IDs?](../glossary#input-ids)\n",
      " |          attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      " |      \n",
      " |              - 1 for tokens that are **not masked**,\n",
      " |              - 0 for tokens that are **masked**.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
      " |              1]`:\n",
      " |      \n",
      " |              - 0 corresponds to a *sentence A* token,\n",
      " |              - 1 corresponds to a *sentence B* token.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      " |              config.n_positions - 1]`.\n",
      " |      \n",
      " |              [What are position IDs?](../glossary#position-ids)\n",
      " |          head_mask (`torch.FloatTensor` of shape `(num_attention_heads,)` or `(n_layer, num_attention_heads)`, *optional*):\n",
      " |              Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      " |      \n",
      " |              - 1 indicates the head is **not masked**,\n",
      " |              - 0 indicates the head is **masked**.\n",
      " |      \n",
      " |          inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\n",
      " |              Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      " |              is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n",
      " |              model's internal embedding lookup matrix.\n",
      " |          output_attentions (`bool`, *optional*):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      " |              tensors for more detail.\n",
      " |          output_hidden_states (`bool`, *optional*):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      " |              more detail.\n",
      " |          return_dict (`bool`, *optional*):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |      \n",
      " |          labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
      " |              `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
      " |              are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
      " |          \n",
      " |      Returns:\n",
      " |          [`transformers.modeling_outputs.CausalLMOutputWithPast`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.CausalLMOutputWithPast`] or a tuple of\n",
      " |          `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      " |          elements depending on the configuration ([`GPTJConfig`]) and inputs.\n",
      " |      \n",
      " |          - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss (for next-token prediction).\n",
      " |          - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      " |          - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      " |            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
      " |      \n",
      " |            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
      " |            `past_key_values` input) to speed up sequential decoding.\n",
      " |          - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      " |            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      " |      \n",
      " |            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      " |          - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      " |            sequence_length)`.\n",
      " |      \n",
      " |            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      " |            heads.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> import torch\n",
      " |      >>> from transformers import GPT2Tokenizer, GPTJForCausalLM\n",
      " |      \n",
      " |      >>> tokenizer = GPT2Tokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n",
      " |      >>> model = GPTJForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gptj\")\n",
      " |      \n",
      " |      >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      " |      >>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
      " |      >>> loss = outputs.loss\n",
      " |      >>> logits = outputs.logits\n",
      " |      ```\n",
      " |  \n",
      " |  get_output_embeddings(self)\n",
      " |      Returns the model's output embeddings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `nn.Module`: A torch module mapping hidden states to vocabulary.\n",
      " |  \n",
      " |  parallelize(self, device_map=None)\n",
      " |      This is an experimental feature and is a subject to change at a moment's notice. Uses a device map to distribute\n",
      " |      attention modules of the model across several devices. If no device map is given, it will evenly distribute blocks\n",
      " |      across all devices.\n",
      " |      \n",
      " |      Args:\n",
      " |          device_map (`Dict[int, list]`, optional, defaults to None):\n",
      " |              A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n",
      " |              automatically mapped to the first device (for esoteric reasons). That means that the first device should\n",
      " |              have fewer attention modules mapped to it than other devices. For reference, the GPT-J models have the\n",
      " |              following number of attention modules:\n",
      " |      \n",
      " |                  - gpt-j-6B: 28\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Here is an example of a device map on a machine with 4 GPUs using gpt-j-6B, which has a total of 28 attention modules:\n",
      " |      model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
      " |      device_map = {\n",
      " |          0: [0, 1, 2, 3, 4, 5, 6],\n",
      " |          1: [7, 8, 9, 10, 11, 12, 13],\n",
      " |          2: [14, 15, 16, 17, 18, 19, 20],\n",
      " |          3: [21, 22, 23, 24, 25, 26, 27],\n",
      " |      }\n",
      " |      model.parallelize(device_map)\n",
      " |      ```\n",
      " |  \n",
      " |  prepare_inputs_for_generation(self, input_ids, past=None, **kwargs)\n",
      " |      Implement in subclasses of [`PreTrainedModel`] for custom behavior to prepare inputs in the generate method.\n",
      " |  \n",
      " |  set_output_embeddings(self, new_embeddings)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from GPTJPreTrainedModel:\n",
      " |  \n",
      " |  base_model_prefix = 'transformer'\n",
      " |  \n",
      " |  config_class = <class 'transformers.models.gptj.configuration_gptj.GPT...\n",
      " |      This is the configuration class to store the configuration of a [`GPTJModel`]. It is used to instantiate a GPT-J\n",
      " |      model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
      " |      defaults will yield a similar configuration to that of the GPT-J\n",
      " |      [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B) architecture. Configuration objects inherit from\n",
      " |      [`PretrainedConfig`] and can be used to control the model outputs. Read the documentation from [`PretrainedConfig`]\n",
      " |      for more information.\n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_size (`int`, *optional*, defaults to 50400):\n",
      " |              Vocabulary size of the GPT-J model. Defines the number of different tokens that can be represented by the\n",
      " |              `inputs_ids` passed when calling [`GPTJModel`].\n",
      " |          n_positions (`int`, *optional*, defaults to 2048):\n",
      " |              The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
      " |              just in case (e.g., 512 or 1024 or 2048).\n",
      " |          n_embd (`int`, *optional*, defaults to 4096):\n",
      " |              Dimensionality of the embeddings and hidden states.\n",
      " |          n_layer (`int`, *optional*, defaults to 28):\n",
      " |              Number of hidden layers in the Transformer encoder.\n",
      " |          n_head (`int`, *optional*, defaults to 16):\n",
      " |              Number of attention heads for each attention layer in the Transformer encoder.\n",
      " |          rotary_dim (`int`, *optional*, defaults to 64):\n",
      " |              Number of dimensions in the embedding that Rotary Position Embedding is applied to.\n",
      " |          n_inner (`int`, *optional*, defaults to None):\n",
      " |              Dimensionality of the inner feed-forward layers. `None` will set it to 4 times n_embd\n",
      " |          activation_function (`str`, *optional*, defaults to `\"gelu_new\"`):\n",
      " |              Activation function, to be selected in the list `[\"relu\", \"silu\", \"gelu\", \"tanh\", \"gelu_new\"]`.\n",
      " |          resid_pdrop (`float`, *optional*, defaults to 0.1):\n",
      " |              The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
      " |          embd_pdrop (`int`, *optional*, defaults to 0.1):\n",
      " |              The dropout ratio for the embeddings.\n",
      " |          attn_pdrop (`float`, *optional*, defaults to 0.1):\n",
      " |              The dropout ratio for the attention.\n",
      " |          layer_norm_epsilon (`float`, *optional*, defaults to 1e-5):\n",
      " |              The epsilon to use in the layer normalization layers.\n",
      " |          initializer_range (`float`, *optional*, defaults to 0.02):\n",
      " |              The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      " |          scale_attn_weights (`bool`, *optional*, defaults to `True`):\n",
      " |              Scale attention weights by dividing by sqrt(hidden_size).\n",
      " |          use_cache (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not the model should return the last key/values attentions (not used by all models).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import GPTJModel, GPTJConfig\n",
      " |      \n",
      " |      >>> # Initializing a GPT-J 6B configuration\n",
      " |      >>> configuration = GPTJConfig()\n",
      " |      \n",
      " |      >>> # Initializing a model from the configuration\n",
      " |      >>> model = GPTJModel(configuration)\n",
      " |      \n",
      " |      >>> # Accessing the model configuration\n",
      " |      >>> configuration = model.config\n",
      " |      ```\n",
      " |  \n",
      " |  is_parallelizable = True\n",
      " |  \n",
      " |  supports_gradient_checkpointing = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  get_input_embeddings(self) -> torch.nn.modules.module.Module\n",
      " |      Returns the model's input embeddings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `nn.Module`: A torch module mapping vocabulary to hidden states.\n",
      " |  \n",
      " |  get_position_embeddings(self) -> Union[torch.nn.modules.sparse.Embedding, Tuple[torch.nn.modules.sparse.Embedding]]\n",
      " |  \n",
      " |  gradient_checkpointing_disable(self)\n",
      " |      Deactivates gradient checkpointing for the current model.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  gradient_checkpointing_enable(self)\n",
      " |      Activates gradient checkpointing for the current model.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      If needed prunes and maybe initializes weights.\n",
      " |  \n",
      " |  post_init(self)\n",
      " |      A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n",
      " |      modules properly initialized (such as weight initialization).\n",
      " |  \n",
      " |  prune_heads(self, heads_to_prune: Dict[int, List[int]])\n",
      " |      Prunes heads of the base model.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          heads_to_prune (`Dict[int, List[int]]`):\n",
      " |              Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads\n",
      " |              to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on\n",
      " |              layer 1 and heads 2 and 3 on layer 2.\n",
      " |  \n",
      " |  push_to_hub(self, repo_path_or_name: Union[str, NoneType] = None, repo_url: Union[str, NoneType] = None, use_temp_dir: bool = False, commit_message: str = 'add model', organization: Union[str, NoneType] = None, private: Union[bool, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str] = '10GB', **model_card_kwargs) -> str\n",
      " |      Upload the model files to the 🤗 Model Hub while synchronizing a local clone of the repo in `repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_path_or_name (`str`, *optional*):\n",
      " |              Can either be a repository name for your model in the Hub or a path to a local folder (in which case\n",
      " |              the repository will have the name of that local folder). If not specified, will default to the name\n",
      " |              given by `repo_url` and a local directory with that name will be created.\n",
      " |          repo_url (`str`, *optional*):\n",
      " |              Specify this in case you want to push to an existing repository in the hub. If unspecified, a new\n",
      " |              repository will be created in your namespace (unless you specify an `organization`) with `repo_name`.\n",
      " |          use_temp_dir (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to clone the distant repo in a temporary directory or in `repo_path_or_name` inside the\n",
      " |              current working directory. This will slow things down if you are making changes in an existing repo\n",
      " |              since you will need to clone the repo before every push.\n",
      " |          commit_message (`str`, *optional*, defaults to `\"add model\"`):\n",
      " |              Message to commit while pushing.\n",
      " |          organization (`str`, *optional*):\n",
      " |              Organization in which you want to push your {object} (you must be a member of this organization).\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private (requires a paying subscription).\n",
      " |          use_auth_token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `transformers-cli login` (stored in `~/.huggingface`). Will default to `True` if\n",
      " |              `repo_url` is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
      " |              The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n",
      " |              lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard\n",
      " |              which will be bigger than `max_shard_size`.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The url of the commit of your {object} in the given repository.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoModel\n",
      " |      \n",
      " |      model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |      # Push the model to your namespace with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |      # *my-finetuned-bert* folder.\n",
      " |      model.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the model to your namespace with the name \"my-finetuned-bert\" with no local clone.\n",
      " |      model.push_to_hub(\"my-finetuned-bert\", use_temp_dir=True)\n",
      " |      \n",
      " |      # Push the model to an organization with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |      # *my-finetuned-bert* folder.\n",
      " |      model.push_to_hub(\"my-finetuned-bert\", organization=\"huggingface\")\n",
      " |      \n",
      " |      # Make a change to an existing repo that has been cloned locally in *my-finetuned-bert*.\n",
      " |      model.push_to_hub(\"my-finetuned-bert\", repo_url=\"https://huggingface.co/sgugger/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  resize_position_embeddings(self, new_num_position_embeddings: int)\n",
      " |  \n",
      " |  resize_token_embeddings(self, new_num_tokens: Union[int, NoneType] = None) -> torch.nn.modules.sparse.Embedding\n",
      " |      Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n",
      " |      \n",
      " |      Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          new_num_tokens (`int`, *optional*):\n",
      " |              The number of new tokens in the embedding matrix. Increasing the size will add newly initialized\n",
      " |              vectors at the end. Reducing the size will remove vectors from the end. If not provided or `None`, just\n",
      " |              returns a pointer to the input tokens `torch.nn.Embedding` module of the model without doing anything.\n",
      " |      \n",
      " |      Return:\n",
      " |          `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n",
      " |  \n",
      " |  retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Union[dict, NoneType] = None, save_function: Callable = <function save at 0x7f55b6822f70>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '10GB', **kwargs)\n",
      " |      Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
      " |      `[`~PreTrainedModel.from_pretrained`]` class method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to which to save. Will be created if it doesn't exist.\n",
      " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the process calling this is the main process or not. Useful when in distributed training like\n",
      " |              TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n",
      " |              the main process to avoid race conditions.\n",
      " |          state_dict (nested dictionary of `torch.Tensor`):\n",
      " |              The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only\n",
      " |              save parts of the model or if special precautions need to be taken when recovering the state dictionary\n",
      " |              of a model (like when using model parallelism).\n",
      " |          save_function (`Callable`):\n",
      " |              The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
      " |              need to replace `torch.save` by another method.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it.\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              Using `push_to_hub=True` will synchronize the repository you are pushing to with `save_directory`,\n",
      " |              which requires `save_directory` to be a local clone of the repo you are pushing to if it's an existing\n",
      " |              folder. Pass along `temp_dir=True` to use a temporary directory instead.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n",
      " |              The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n",
      " |              lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard\n",
      " |              which will be bigger than `max_shard_size`.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |          kwargs:\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  set_input_embeddings(self, value: torch.nn.modules.module.Module)\n",
      " |      Set model's input embeddings.\n",
      " |      \n",
      " |      Args:\n",
      " |          value (`nn.Module`): A module mapping vocabulary to hidden states.\n",
      " |  \n",
      " |  tie_weights(self)\n",
      " |      Tie the weights between the input embeddings and the output embeddings.\n",
      " |      \n",
      " |      If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n",
      " |      weights instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, **kwargs) from builtins.type\n",
      " |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      " |      \n",
      " |      The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
      " |      the model, you should first set it back in training mode with `model.train()`.\n",
      " |      \n",
      " |      The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
      " |      pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
      " |      task.\n",
      " |      \n",
      " |      The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
      " |      weights are discarded.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
      " |                    Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      " |                    user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      " |                  - A path to a *directory* containing model weights saved using\n",
      " |                    [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
      " |                  - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
      " |                    this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
      " |                    `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
      " |                    PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      " |                  - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n",
      " |                    `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n",
      " |                    `True`.\n",
      " |                  - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
      " |                    arguments `config` and `state_dict`).\n",
      " |          model_args (sequence of positional arguments, *optional*):\n",
      " |              All remaining positional arguments will be passed to the underlying model's `__init__` method.\n",
      " |          config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - an instance of a class derived from [`PretrainedConfig`],\n",
      " |                  - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n",
      " |      \n",
      " |              Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
      " |              be automatically loaded when:\n",
      " |      \n",
      " |                  - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
      " |                    model).\n",
      " |                  - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
      " |                    save directory.\n",
      " |                  - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
      " |                    configuration JSON file named *config.json* is found in the directory.\n",
      " |          state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
      " |              A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
      " |      \n",
      " |              This option can be used if you want to create a model from a pretrained configuration but load your own\n",
      " |              weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
      " |              [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          from_tf (`bool`, *optional*, defaults to `False`):\n",
      " |              Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
      " |              `pretrained_model_name_or_path` argument).\n",
      " |          from_flax (`bool`, *optional*, defaults to `False`):\n",
      " |              Load the model weights from a Flax checkpoint save file (see docstring of\n",
      " |              `pretrained_model_name_or_path` argument).\n",
      " |          ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n",
      " |              as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n",
      " |              checkpoint with 3 labels).\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `transformers-cli login` (stored in `~/.huggingface`).\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
      " |              problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
      " |              Please refer to the mirror site for more information.\n",
      " |          _fast_init(`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to disable fast initialization.\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\n",
      " |              4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\n",
      " |              [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |          low_cpu_mem_usage(`bool`, *optional*, defaults to `False`):\n",
      " |              Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              This is an experimental feature and a subject to change at any moment.\n",
      " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n",
      " |              will be automatically derived from the model's weights.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
      " |              `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
      " |              automatically loaded:\n",
      " |      \n",
      " |                  - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
      " |                    underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
      " |                    already been done)\n",
      " |                  - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
      " |                    initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
      " |                    corresponds to a configuration attribute will be used to override said attribute with the\n",
      " |                    supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
      " |                    will be passed to the underlying model's `__init__` function.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Passing `use_auth_token=True`` is required when you want to use a private model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n",
      " |      use this method in a firewalled environment.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import BertConfig, BertModel\n",
      " |      \n",
      " |      >>> # Download model and configuration from huggingface.co and cache.\n",
      " |      >>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      " |      >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n",
      " |      >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n",
      " |      >>> # Update configuration during loading.\n",
      " |      >>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
      " |      >>> assert model.config.output_attentions == True\n",
      " |      >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
      " |      >>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n",
      " |      >>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n",
      " |      >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
      " |      >>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n",
      " |      ```\n",
      " |      \n",
      " |      * `low_cpu_mem_usage` algorithm:\n",
      " |      \n",
      " |      This is an experimental function that loads the model using ~1x model size CPU memory\n",
      " |      \n",
      " |      Here is how it works:\n",
      " |      \n",
      " |      1. save which state_dict keys we have\n",
      " |      2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n",
      " |      3. after the model has been instantiated switch to the meta device all params/buffers that\n",
      " |      are going to be replaced from the loaded state_dict\n",
      " |      4. load state_dict 2nd time\n",
      " |      5. replace the params/buffers from the state_dict\n",
      " |      \n",
      " |      Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoModel') from builtins.type\n",
      " |      Register this class with a given auto class. This should only be used for custom models as the ones in the\n",
      " |      library are already mapped with an auto class.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoModel\"`):\n",
      " |              The auto class to register this new model with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  base_model\n",
      " |      `torch.nn.Module`: The main body of the model.\n",
      " |  \n",
      " |  dummy_inputs\n",
      " |      `Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n",
      " |  \n",
      " |  framework\n",
      " |      :str: Identifies that this is a PyTorch model.\n",
      " |  \n",
      " |  is_gradient_checkpointing\n",
      " |      Whether gradient checkpointing is activated for this model or not.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  main_input_name = 'input_ids'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |          or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      " |  \n",
      " |  dump_patches = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  add_memory_hooks(self)\n",
      " |      Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n",
      " |      \n",
      " |      Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero\n",
      " |      with `model.reset_memory_hooks_state()`.\n",
      " |  \n",
      " |  estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> int\n",
      " |      Helper function to estimate the total number of tokens from the model inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (`dict`): The model inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The total number of tokens.\n",
      " |  \n",
      " |  floating_point_ops(self, input_dict: Dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True) -> int\n",
      " |      Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n",
      " |      batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n",
      " |      tokens (valid if `12 * d_model << sequence_length`) as laid out in [this\n",
      " |      paper](https://arxiv.org/pdf/2001.08361.pdf) section 2.1. Should be overridden for transformers with parameter\n",
      " |      re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size (`int`):\n",
      " |              The batch size for the forward pass.\n",
      " |      \n",
      " |          sequence_length (`int`):\n",
      " |              The number of tokens in each line of the batch.\n",
      " |      \n",
      " |          exclude_embeddings (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to count embedding and softmax operations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of floating-point operations.\n",
      " |  \n",
      " |  get_extended_attention_mask(self, attention_mask: torch.Tensor, input_shape: Tuple[int], device: <property object at 0x7f5473bcb130> = None) -> torch.Tensor\n",
      " |      Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          attention_mask (`torch.Tensor`):\n",
      " |              Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
      " |          input_shape (`Tuple[int]`):\n",
      " |              The shape of the input to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n",
      " |  \n",
      " |  get_head_mask(self, head_mask: Union[torch.Tensor, NoneType], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor\n",
      " |      Prepare the head mask if needed.\n",
      " |      \n",
      " |      Args:\n",
      " |          head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n",
      " |              The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n",
      " |          num_hidden_layers (`int`):\n",
      " |              The number of hidden layers in the model.\n",
      " |          is_attention_chunked: (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the attentions scores are computed by chunks or not.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `torch.Tensor` with shape `[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or list with\n",
      " |          `[None]` for each layer.\n",
      " |  \n",
      " |  invert_attention_mask(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor\n",
      " |      Invert an attention mask (e.g., switches 0. and 1.).\n",
      " |      \n",
      " |      Args:\n",
      " |          encoder_attention_mask (`torch.Tensor`): An attention mask.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `torch.Tensor`: The inverted attention mask.\n",
      " |  \n",
      " |  num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int\n",
      " |      Get number of (optionally, trainable or non-embeddings) parameters in the module.\n",
      " |      \n",
      " |      Args:\n",
      " |          only_trainable (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return only the number of trainable parameters\n",
      " |      \n",
      " |          exclude_embeddings (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return only the number of non-embeddings parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of parameters.\n",
      " |  \n",
      " |  reset_memory_hooks_state(self)\n",
      " |      Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory_hooks`]).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  device\n",
      " |      `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n",
      " |      device).\n",
      " |  \n",
      " |  dtype\n",
      " |      `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.generation_utils.GenerationMixin:\n",
      " |  \n",
      " |  adjust_logits_during_generation(self, logits: torch.FloatTensor, **kwargs) -> torch.FloatTensor\n",
      " |      Implement in subclasses of [`PreTrainedModel`] for custom behavior to adjust the logits in the generate method.\n",
      " |  \n",
      " |  beam_sample(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = None, logits_warper: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, max_length: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, **model_kwargs) -> Union[transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **beam search multinomial\n",
      " |      sampling** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (`BeamScorer`):\n",
      " |              A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
      " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
      " |              sampling at each generation step.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.BeamSampleDecoderOnlyOutput`], [`~generation_utils.BeamSampleEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSampleDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.BeamSampleEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     TopKLogitsWarper,\n",
      " |      ...     TemperatureLogitsWarper,\n",
      " |      ...     BeamSearchScorer,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # lets run beam search using 3 beams\n",
      " |      >>> num_beams = 3\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = BeamSearchScorer(\n",
      " |      ...     batch_size=1,\n",
      " |      ...     max_length=model.config.max_length,\n",
      " |      ...     num_beams=num_beams,\n",
      " |      ...     device=model.device,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]\n",
      " |      ... )\n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_warper = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         TopKLogitsWarper(50),\n",
      " |      ...         TemperatureLogitsWarper(0.7),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.beam_sample(\n",
      " |      ...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt bist du?']\n",
      " |      ```\n",
      " |  \n",
      " |  beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = None, max_length: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, **model_kwargs) -> Union[transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **beam search decoding** and\n",
      " |      can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (`BeamScorer`):\n",
      " |              An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`generation_utilsBeamSearchDecoderOnlyOutput`], [`~generation_utils.BeamSearchEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.BeamSearchEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     BeamSearchScorer,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |      >>> # lets run beam search using 3 beams\n",
      " |      >>> num_beams = 3\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = BeamSearchScorer(\n",
      " |      ...     batch_size=1,\n",
      " |      ...     num_beams=num_beams,\n",
      " |      ...     device=model.device,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt bist du?']\n",
      " |      ```\n",
      " |  \n",
      " |  compute_transition_beam_scores(self, sequences: torch.Tensor, scores: Tuple[torch.Tensor], beam_indices: torch.Tensor, eos_token_id: int = None)\n",
      " |      compute the transition probabilities of sequences given generation\n",
      " |      scores and beam indices\n",
      " |  \n",
      " |  constrained_beam_search(self, input_ids: torch.LongTensor, constrained_beam_scorer: transformers.generation_beam_search.ConstrainedBeamSearchScorer, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = None, max_length: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = None, **model_kwargs) -> Union[transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **constrained beam search\n",
      " |      decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          constrained_beam_scorer (`ConstrainedBeamSearchScorer`):\n",
      " |              A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation, while satisfying a list of positive constraints. For more information, the\n",
      " |              documentation of [`ConstrainedBeamSearchScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
      " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
      " |              sampling at each generation step.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`generation_utilsBeamSearchDecoderOnlyOutput`], [`~generation_utils.BeamSearchEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.BeamSearchEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     ConstrainedBeamSearchScorer,\n",
      " |      ...     PhrasalConstraint,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |      >>> # lets run beam search using 3 beams\n",
      " |      >>> num_beams = 3\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> constraint_str = \"Sie\"\n",
      " |      >>> constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token\n",
      " |      >>> constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]\n",
      " |      \n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = ConstrainedBeamSearchScorer(\n",
      " |      ...     batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.constrained_beam_search(\n",
      " |      ...     input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt sind Sie?']\n",
      " |      ```\n",
      " |  \n",
      " |  generate(self, inputs: Union[torch.Tensor, NoneType] = None, max_length: Union[int, NoneType] = None, min_length: Union[int, NoneType] = None, do_sample: Union[bool, NoneType] = None, early_stopping: Union[bool, NoneType] = None, num_beams: Union[int, NoneType] = None, temperature: Union[float, NoneType] = None, top_k: Union[int, NoneType] = None, top_p: Union[float, NoneType] = None, typical_p: Union[float, NoneType] = None, repetition_penalty: Union[float, NoneType] = None, bad_words_ids: Union[Iterable[int], NoneType] = None, force_words_ids: Union[Iterable[int], Iterable[Iterable[int]], NoneType] = None, bos_token_id: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, length_penalty: Union[float, NoneType] = None, no_repeat_ngram_size: Union[int, NoneType] = None, encoder_no_repeat_ngram_size: Union[int, NoneType] = None, num_return_sequences: Union[int, NoneType] = None, max_time: Union[float, NoneType] = None, max_new_tokens: Union[int, NoneType] = None, decoder_start_token_id: Union[int, NoneType] = None, use_cache: Union[bool, NoneType] = None, num_beam_groups: Union[int, NoneType] = None, diversity_penalty: Union[float, NoneType] = None, prefix_allowed_tokens_fn: Union[Callable[[int, torch.Tensor], List[int]], NoneType] = None, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = [], renormalize_logits: Union[bool, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = [], constraints: Union[List[transformers.generation_beam_constraints.Constraint], NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, forced_bos_token_id: Union[int, NoneType] = None, forced_eos_token_id: Union[int, NoneType] = None, remove_invalid_values: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, exponential_decay_length_penalty: Union[Tuple[Union[int, float]], NoneType] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head. The method supports the following\n",
      " |      generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
      " |      \n",
      " |          - *greedy decoding* by calling [`~generation_utils.GenerationMixin.greedy_search`] if `num_beams=1` and\n",
      " |            `do_sample=False`.\n",
      " |          - *multinomial sampling* by calling [`~generation_utils.GenerationMixin.sample`] if `num_beams=1` and\n",
      " |            `do_sample=True`.\n",
      " |          - *beam-search decoding* by calling [`~generation_utils.GenerationMixin.beam_search`] if `num_beams>1` and\n",
      " |            `do_sample=False`.\n",
      " |          - *beam-search multinomial sampling* by calling [`~generation_utils.GenerationMixin.beam_sample`] if\n",
      " |            `num_beams>1` and `do_sample=True`.\n",
      " |          - *diverse beam-search decoding* by calling [`~generation_utils.GenerationMixin.group_beam_search`], if\n",
      " |            `num_beams>1` and `num_beam_groups>1`.\n",
      " |          - *constrained beam-search decoding* by calling\n",
      " |            [`~generation_utils.GenerationMixin.constrained_beam_search`], if `constraints!=None` or\n",
      " |            `force_words_ids!=None`.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as\n",
      " |      defined in the model's config (`config.json`) which in turn defaults to the\n",
      " |      [`~modeling_utils.PretrainedConfig`] of the model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Most of these parameters are explained in more detail in [this blog\n",
      " |      post](https://huggingface.co/blog/how-to-generate).\n",
      " |      \n",
      " |      Parameters:\n",
      " |          inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      " |              The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      " |              method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      " |              should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      " |              `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      " |          max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
      " |              The maximum length of the sequence to be generated.\n",
      " |          max_new_tokens (`int`, *optional*, defaults to None):\n",
      " |              The maximum numbers of tokens to generate, ignore the current number of tokens. Use either\n",
      " |              `max_new_tokens` or `max_length` but not both, they serve the same purpose.\n",
      " |          min_length (`int`, *optional*, defaults to 10):\n",
      " |              The minimum length of the sequence to be generated.\n",
      " |          do_sample (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to use sampling ; use greedy decoding otherwise.\n",
      " |          early_stopping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
      " |          num_beams (`int`, *optional*, defaults to 1):\n",
      " |              Number of beams for beam search. 1 means no beam search.\n",
      " |          temperature (`float`, *optional*, defaults to 1.0):\n",
      " |              The value used to module the next token probabilities.\n",
      " |          top_k (`int`, *optional*, defaults to 50):\n",
      " |              The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      " |          top_p (`float`, *optional*, defaults to 1.0):\n",
      " |              If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
      " |              are kept for generation.\n",
      " |          repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
      " |              The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
      " |              paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          bos_token_id (`int`, *optional*):\n",
      " |              The id of the *beginning-of-sequence* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          length_penalty (`float`, *optional*, defaults to 1.0):\n",
      " |               Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length.\n",
      " |               0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer\n",
      " |               sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n",
      " |          no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
      " |              If set to int > 0, all ngrams of that size can only occur once.\n",
      " |          encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
      " |              If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
      " |              `decoder_input_ids`.\n",
      " |          bad_words_ids(`List[List[int]]`, *optional*):\n",
      " |              List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
      " |              should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
      " |              add_special_tokens=False).input_ids`.\n",
      " |          force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
      " |              List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
      " |              list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
      " |              this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
      " |              where one can allow different forms of each word.\n",
      " |          num_return_sequences(`int`, *optional*, defaults to 1):\n",
      " |              The number of independently computed returned sequences for each element in the batch.\n",
      " |          max_time(`float`, *optional*, defaults to None):\n",
      " |              The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
      " |              finish the current pass after allocated time has been passed.\n",
      " |          attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      " |              Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
      " |              that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
      " |              as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
      " |          decoder_start_token_id (`int`, *optional*):\n",
      " |              If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
      " |          use_cache: (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      " |              speed up decoding.\n",
      " |          num_beam_groups (`int`, *optional*, defaults to 1):\n",
      " |              Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
      " |              beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
      " |          diversity_penalty (`float`, *optional*, defaults to 0.0):\n",
      " |              This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
      " |              at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
      " |              enabled.\n",
      " |          prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      " |              If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      " |              provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      " |              `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      " |              on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      " |              for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      " |              Retrieval](https://arxiv.org/abs/2010.00904).\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |               Custom logits processors that complement the default logits processors built from arguments and a\n",
      " |               model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
      " |               config an error is thrown. This feature is intended for advanced users.\n",
      " |          renormalize_logits: (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
      " |              custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
      " |              score logits are normalized but some logit processors or warpers break the normalization.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |               Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
      " |               model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
      " |               model's config an error is thrown. This feature is intended for advanced users.\n",
      " |          constraints (`List[Constraint]`, *optional*):\n",
      " |               Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
      " |               of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          forced_bos_token_id (`int`, *optional*):\n",
      " |              The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
      " |              for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
      " |              the target language token.\n",
      " |          forced_eos_token_id (`int`, *optional*):\n",
      " |              The id of the token to force as the last generated token when `max_length` is reached.\n",
      " |          remove_invalid_values (`bool`, *optional*):\n",
      " |              Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
      " |              crash. Note that using `remove_invalid_values` can slow down generation.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n",
      " |              This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
      " |              generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
      " |              where penalty starts and `decay_factor` represents the factor of exponential decay\n",
      " |      \n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
      " |              is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
      " |              should be prefixed with *decoder_*.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      " |          or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      " |      \n",
      " |              If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      " |              [`~utils.ModelOutput`] types are:\n",
      " |      \n",
      " |                  - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
      " |                  - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
      " |                  - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
      " |                  - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
      " |      \n",
      " |              If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      " |              [`~utils.ModelOutput`] types are:\n",
      " |      \n",
      " |                  - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
      " |                  - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
      " |                  - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
      " |                  - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      Greedy Decoding:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> prompt = \"Today I believe we can finally\"\n",
      " |      >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # generate up to 30 tokens\n",
      " |      >>> outputs = model.generate(input_ids, do_sample=False, max_length=30)\n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n']\n",
      " |      ```\n",
      " |      \n",
      " |      Multinomial Sampling:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> prompt = \"Today I believe we can finally\"\n",
      " |      >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # sample up to 30 tokens\n",
      " |      >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
      " |      >>> outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']\n",
      " |      ```\n",
      " |      \n",
      " |      Beam-search decoding:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
      " |      \n",
      " |      >>> sentence = \"Paris is one of the densest populated areas in Europe.\"\n",
      " |      >>> input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> outputs = model.generate(input_ids)\n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\n",
      " |      ```\n",
      " |  \n",
      " |  greedy_search(self, input_ids: torch.LongTensor, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = None, max_length: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **greedy decoding** and can be\n",
      " |      used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |      \n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n",
      " |              If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.GreedySearchDecoderOnlyOutput`], [`~generation_utils.GreedySearchEncoderDecoderOutput`]\n",
      " |          or `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.GreedySearchDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.GreedySearchEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForCausalLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     StoppingCriteriaList,\n",
      " |      ...     MaxLengthCriteria,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
      " |      >>> model.config.pad_token_id = model.config.eos_token_id\n",
      " |      \n",
      " |      >>> input_prompt = \"It might be possible to\"\n",
      " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
      " |      \n",
      " |      >>> outputs = model.greedy_search(\n",
      " |      ...     input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      [\"It might be possible to get a better understanding of the nature of the problem, but it's not\"]\n",
      " |      ```\n",
      " |  \n",
      " |  group_beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = None, max_length: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, **model_kwargs)\n",
      " |      Generates sequences of token ids for models with a language modeling head using **diverse beam search\n",
      " |      decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (`BeamScorer`):\n",
      " |              An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
      " |              sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |      \n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs that will be forwarded to the `forward` function of the model. If\n",
      " |              model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`], [`~generation_utils.BeamSearchEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.BeamSearchDecoderOnlyOutput`] if [`~generation_utils.BeamSearchDecoderOnlyOutput`] if\n",
      " |          `model.config.is_encoder_decoder=False` and `return_dict_in_generate=True` or a\n",
      " |          [`~generation_utils.BeamSearchEncoderDecoderOutput`] if `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForSeq2SeqLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     HammingDiversityLogitsProcessor,\n",
      " |      ...     BeamSearchScorer,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |      >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |      >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |      >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |      >>> # lets run diverse beam search using 6 beams\n",
      " |      >>> num_beams = 6\n",
      " |      >>> # define decoder start token ids\n",
      " |      >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |      >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |      >>> # add encoder_outputs to model keyword arguments\n",
      " |      >>> model_kwargs = {\n",
      " |      ...     \"encoder_outputs\": model.get_encoder()(\n",
      " |      ...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
      " |      ...     )\n",
      " |      ... }\n",
      " |      \n",
      " |      >>> # instantiate beam scorer\n",
      " |      >>> beam_scorer = BeamSearchScorer(\n",
      " |      ...     batch_size=1,\n",
      " |      ...     max_length=model.config.max_length,\n",
      " |      ...     num_beams=num_beams,\n",
      " |      ...     device=model.device,\n",
      " |      ...     num_beam_groups=3,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n",
      " |      ...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> outputs = model.group_beam_search(\n",
      " |      ...     input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Wie alt bist du?']\n",
      " |      ```\n",
      " |  \n",
      " |  sample(self, input_ids: torch.LongTensor, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = None, logits_warper: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = None, max_length: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, **model_kwargs) -> Union[transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and\n",
      " |      can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      " |              used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      " |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      " |              An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
      " |              used to tell if the generation loop should stop.\n",
      " |          logits_warper (`LogitsProcessorList`, *optional*):\n",
      " |              An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used\n",
      " |              to warp the prediction score distribution of the language modeling head applied before multinomial\n",
      " |              sampling at each generation step.\n",
      " |          max_length (`int`, *optional*, defaults to 20):\n",
      " |              **DEPRECATED**. Use `logits_processor` or `stopping_criteria` directly to cap the number of generated\n",
      " |              tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (`int`, *optional*):\n",
      " |              The id of the *padding* token.\n",
      " |          eos_token_id (`int`, *optional*):\n",
      " |              The id of the *end-of-sequence* token.\n",
      " |          output_attentions (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      " |          return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      " |          synced_gpus (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n",
      " |              an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`~generation_utils.SampleDecoderOnlyOutput`], [`~generation_utils.SampleEncoderDecoderOutput`] or\n",
      " |          `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          [`~generation_utils.SampleDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
      " |          `return_dict_in_generate=True` or a [`~generation_utils.SampleEncoderDecoderOutput`] if\n",
      " |          `model.config.is_encoder_decoder=True`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import (\n",
      " |      ...     AutoTokenizer,\n",
      " |      ...     AutoModelForCausalLM,\n",
      " |      ...     LogitsProcessorList,\n",
      " |      ...     MinLengthLogitsProcessor,\n",
      " |      ...     TopKLogitsWarper,\n",
      " |      ...     TemperatureLogitsWarper,\n",
      " |      ...     StoppingCriteriaList,\n",
      " |      ...     MaxLengthCriteria,\n",
      " |      ... )\n",
      " |      >>> import torch\n",
      " |      \n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
      " |      >>> model.config.pad_token_id = model.config.eos_token_id\n",
      " |      \n",
      " |      >>> input_prompt = \"Today is a beautiful day, and\"\n",
      " |      >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_processor = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      >>> # instantiate logits processors\n",
      " |      >>> logits_warper = LogitsProcessorList(\n",
      " |      ...     [\n",
      " |      ...         TopKLogitsWarper(50),\n",
      " |      ...         TemperatureLogitsWarper(0.7),\n",
      " |      ...     ]\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
      " |      \n",
      " |      >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
      " |      >>> outputs = model.sample(\n",
      " |      ...     input_ids,\n",
      " |      ...     logits_processor=logits_processor,\n",
      " |      ...     logits_warper=logits_warper,\n",
      " |      ...     stopping_criteria=stopping_criteria,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      " |      ['Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the']\n",
      " |      ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTJConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-j-6B\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
