{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Wikipedia\n",
    "We need to have the wikipedia articles presented as a series of tokens of the dimension of our model. BottleneckT5 is a good autoencoder that reduces sentences of 512 or less tokens into vectors of various sizes. I was happiest with the performance of the large model, which gives vectors that are 1024 long. If we make this the dimension of our model then we can preprocess wikipedia into a series of vectors 1024 long. BottleneckT5 will encode multiple sentences provided the total number of tokens are less than 512. How we divide the data will determine how our model works. If we assume single whole sentences in then we should expect each output to represent one sentence. As we are moving from models that use part words as inputs to whole sentences let's be conservative first and not encode multiple sentences. This corresponds with my notion that we are creating a model that works with ideas, rather than words. That means we need to identify thae sentences in our articles and then create the corresponding vectors. This notebook will go through the process of doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List\n",
    "\n",
    "class BottleneckT5Autoencoder:\n",
    "    def __init__(self, model_path: str, device='cpu'):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=512)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, text: List[str]) -> List[List[float]]:\n",
    "\n",
    "        # big batches are causing us to run out of memory. Limit the size\n",
    "        embeddings = list()\n",
    "        for i in range(0, len(text), 100):\n",
    "            end = i + 100\n",
    "            if end > len(text):\n",
    "                end = len(text)\n",
    "            batch = text[i:end]\n",
    "        \n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "            decoder_inputs = self.tokenizer('', return_tensors='pt').to(self.device)\n",
    "            embeddings.extend(self.model(\n",
    "                    **inputs,\n",
    "                    decoder_input_ids=decoder_inputs['input_ids'],\n",
    "                    encode_only=True,\n",
    "                ).to('cpu').tolist())\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_from_latent(self, latent: List[float], max_length=512, temperature=1.0) -> str:\n",
    "        dummy_text = ['.']\n",
    "        dummy = torch.tensor(self.embed(dummy_text)).to(device)\n",
    "        latent = torch.tensor(latent).to(device)\n",
    "        perturb_vector = latent - dummy\n",
    "        self.model.perturb_vector = perturb_vector\n",
    "        input_ids = self.tokenizer(dummy_text, return_tensors='pt').to(self.device).input_ids\n",
    "        output = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "autoencoder = BottleneckT5Autoencoder(model_path='thesephist/contra-bottleneck-t5-large-wikipedia', device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we check the performance of BottleneckT5 on a single sentence. Random sentences are given that the model could not have possibly seen before to prove that the model has found a good generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am John Oates, and I am a software engineer at a large technology company.\n",
      "Transformers are a neat way to generate pattern recognition in sequences.\n",
      "Religion is the study of why nature is the way it is, while science studies how things work and how they happened.\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    'My name is John Oates, and I am a software engineer at a large technology company.',\n",
    "    'Transformers are a neat way to generate pattern recognition in sequences.',\n",
    "    'Religion is the study of why nature is the way it is, while science studies how nature works and when it happened.',\n",
    "]\n",
    "\n",
    "embedding = autoencoder.embed(texts)\n",
    "for i in range(len(embedding)):\n",
    "    reconstruction = autoencoder.generate_from_latent(embedding[i])\n",
    "    print(reconstruction)\n",
    "\n",
    "print(type(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to proceed with the encoding we first want to test the process on a small subset. Lets use the first 10 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/datasets/load.py:1429: FutureWarning: The repository for olm/wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/olm/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wiki = load_dataset(\"olm/wikipedia\", language=\"en\", date=\"20240201\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Skim off the first 10 articles for testing\n",
    "test_wiki = wiki['train'][:10]\n",
    "test_wiki = Dataset.from_dict(test_wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['id', 'url', 'title', 'text']}\n",
      "['id', 'url', 'title', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(wiki.column_names)\n",
    "print(test_wiki.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2070.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_sentences(article):\n",
    "    # split the article into sentences\n",
    "    sentences = re.split(r'(?<=[.!?;:\\n\\r])\\s+', article)\n",
    "    # remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "    # remove sentences that are too long by filtering out sentences with more than 400 words\n",
    "    sentences = [s for s in sentences if len(s.split()) <= 400]\n",
    "    return sentences\n",
    "\n",
    "test_wiki_sentences = test_wiki.map(lambda x: {'sentences': get_sentences(x['text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 13.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings_batch(sentences):\n",
    "    return autoencoder.embed(sentences)\n",
    "\n",
    "test_wiki_embeddings = test_wiki_sentences.map(lambda x: {'embeddings': get_embeddings_batch(x['sentences'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'url', 'title', 'text', 'sentences', 'embeddings']\n",
      "The Vickers Vagabond was Vickers' entrant for the second Lympne light aircraft competition, held in 1924.\n",
      "tensor([ 0.0124,  0.0364, -0.0052,  ..., -0.0788,  0.0109,  0.1947],\n",
      "       device='cuda:0')\n",
      "The Vickers Vagabond was a Vickers' entry for the second Lympne light aircraft competition, held in 1924.\n",
      "-----------------\n",
      "It was a conventional small biplane, with a very unusual method of trimming.\n",
      "tensor([-0.1697,  0.0039, -0.0185,  ...,  0.0743,  0.0542, -0.0175],\n",
      "       device='cuda:0')\n",
      "It was a conventional small biplane, with a very unusual method of trimming.\n",
      "-----------------\n",
      "It was eliminated from the trials at an early stage and only one was built.\n",
      "tensor([-1.2169e-01,  5.5488e-02,  6.1400e-03,  ..., -2.9306e-02,\n",
      "         2.9215e-02, -6.4560e-05], device='cuda:0')\n",
      "It was eliminated from the trials at an early stage and only one was built.\n",
      "-----------------\n",
      "Development\n",
      "Following the first Lympne trials held in 1923 for single-seat motor-gliders, the Air Ministry organised a similar event in 1924, this time for low-powered two-seat aircraft.\n",
      "tensor([ 0.0015, -0.0930, -0.0062,  ..., -0.0216, -0.0501,  0.1220],\n",
      "       device='cuda:0')\n",
      "Following the first Lympne trials held in 1923, the Ministry of Aeronautics organised a second competition in 1924 for single-seat low-winged aircraft, this time for two-seat light aircraft.\n",
      "-----------------\n",
      "The engine capacity limit was set at 1,100 cc.\n",
      "tensor([-0.0577, -0.0007,  0.0664,  ...,  0.0414, -0.0023,  0.0087],\n",
      "       device='cuda:0')\n",
      "The engine capacity limit was set at 1,100 cc.\n",
      "-----------------\n",
      "and, as before, the wings had to fold for easy transport and storage.\n",
      "tensor([-0.0581, -0.0207, -0.0839,  ..., -0.0044,  0.0012, -0.0094],\n",
      "       device='cuda:0')\n",
      "and, as before, the wings had to fold up for easy transport and storage.\n",
      "-----------------\n",
      "The trials took place between 29 September and 4 October.\n",
      "tensor([-0.0807, -0.0531,  0.0972,  ..., -0.0157,  0.0682, -0.0365],\n",
      "       device='cuda:0')\n",
      "The trials took place between 29 September and 4 October.\n",
      "-----------------\n",
      "Several companies built aircraft for them, including the Blackburn Bluebird, Hawker Cygnet, Supermarine Sparrow and two from Westland, the Woodpigeon and Widgeon.\n",
      "tensor([ 0.0154, -0.0497,  0.1422,  ...,  0.0060, -0.0088, -0.0189],\n",
      "       device='cuda:0')\n",
      "Several companies built aircraft for them, including the Blackburn Bluebird, Supermarine Spitfire, Hawker Cygnet, and two from Westland, the Woodsylvania and Widgeon.\n",
      "-----------------\n",
      "The Type 98 Vagabond was Vickers' entry.\n",
      "tensor([ 0.0368, -0.0262, -0.0173,  ...,  0.0366,  0.1551,  0.1040],\n",
      "       device='cuda:0')\n",
      "The Type 98 Vagabond was Vickers' entry.\n",
      "-----------------\n",
      "It was a single-bay, wire-braced biplane with wings of constant chord except towards the rounded trailing tips.\n",
      "tensor([-0.0536,  0.0528, -0.1070,  ...,  0.1523, -0.0792,  0.1130],\n",
      "       device='cuda:0')\n",
      "It was a single-bay, wire-braced biplane with constant chord except towards the tips of the rounded trailing edges.\n",
      "-----------------\n",
      "The wings had equal span and carried marked stagger.\n",
      "tensor([ 0.0020, -0.0949,  0.0282,  ...,  0.0795, -0.0990,  0.0383],\n",
      "       device='cuda:0')\n",
      "The wings were equal span and showed marked stagger.\n",
      "-----------------\n",
      "There were ailerons on both upper and lower wings, with flaps inboard on the lower wings which could be folded to assist wing-folding.\n",
      "tensor([ 0.0339, -0.0355,  0.0077,  ..., -0.0193,  0.0285, -0.0390],\n",
      "       device='cuda:0')\n",
      "There were ailerons on both upper and lower wings, with flaps on the underwing which could be folded inwards to assist in wing-loading.\n",
      "-----------------\n",
      "The pilot and passenger sat in open cockpits, the latter under the upper wing.\n",
      "tensor([ 0.0019, -0.0031,  0.0047,  ..., -0.0640, -0.0294, -0.0045],\n",
      "       device='cuda:0')\n",
      "The pilot and passenger sat in open cockpits, the latter under the upper wing.\n",
      "-----------------\n",
      "The pilot's upward view was enhanced by a small cutout in the trailing edge of the top wing.\n",
      "tensor([ 0.0177,  0.0070,  0.0193,  ...,  0.0333, -0.1084,  0.0220],\n",
      "       device='cuda:0')\n",
      "The pilot's upward view was enhanced by a small cutout in the trailing edge of the top wing.\n",
      "-----------------\n",
      "The fuselage had a more rounded cross-section than that of the earlier Viget, Vickers' single-seat entry to the 1923 competition, extending a little below the lower wing.\n",
      "tensor([ 0.0091, -0.0485, -0.0587,  ...,  0.0077, -0.0789,  0.0841],\n",
      "       device='cuda:0')\n",
      "The fuselage had a more rounded cross-section than the earlier Viget, Vickers' entry to the 1923 single-seater competition, extending slightly below the lower wing.\n",
      "-----------------\n",
      "The 32 hp (24 kW)  Bristol Cherub III flat twin engine was mounted in a smooth nose with the finned cylinders exposed for air cooling.\n",
      "tensor([ 0.0564,  0.0520, -0.0771,  ...,  0.0845, -0.0603,  0.1074],\n",
      "       device='cuda:0')\n",
      "The Bristol Cherub III (12 cylinders 32 hp) flat twin engine was mounted in a smooth nose with the cooled air intakes exposed for cooling.\n",
      "-----------------\n",
      "The horizontal tail was similar to that of the Viget, but the fin and rudder were much more rounded.\n",
      "tensor([-0.1684, -0.1455,  0.0056,  ...,  0.0097, -0.0611,  0.0525],\n",
      "       device='cuda:0')\n",
      "The horizontal tail was much similar to the Viget, but the fin and rudder were much more rounded.\n",
      "-----------------\n",
      "Because of the stagger, the mainwheels were in front of the lower wing, braced to the lower fuselage logeron aft to the front wing spar and forward to a point roughly below the upper wing leading edge.\n",
      "tensor([-0.0244, -0.1334,  0.0395,  ..., -0.0652, -0.0602,  0.0694],\n",
      "       device='cuda:0')\n",
      "Because of the stagger, the mainwheels were inboard of the lower wing, braced to the lower wing spar and forward to the forward mainplane aft lateral to the lower tailfin, set well back against a line extending aft below the upper leading edge of the wing root.\n",
      "-----------------\n",
      "A most unusual feature of the Vagabond was the method of longitudinal trimming.\n",
      "tensor([-0.1369,  0.0281,  0.0197,  ...,  0.0903,  0.0939,  0.0441],\n",
      "       device='cuda:0')\n",
      "A most unusual feature of the Vagabond was the method of longitudinal trimming.\n",
      "-----------------\n",
      "Rather than changing the angle of the tailplane with respect to the fuselage, the whole rear part of the fuselage was hinged just ahead of the lower wing's trailing edge.\n",
      "tensor([-0.0391, -0.0678, -0.0552,  ..., -0.1656, -0.0331,  0.0289],\n",
      "       device='cuda:0')\n",
      "Rather than changing the angle of the tailplane with respect to the fuselage, the whole upper part of the wing was hinged directly next to the lower wing's trailing edge.\n",
      "-----------------\n",
      "This was controlled via a handwheel between the two cockpits;\n",
      "tensor([-0.1049, -0.0651, -0.0450,  ..., -0.0545, -0.0439,  0.0019],\n",
      "       device='cuda:0')\n",
      "This was controlled by a handwheel between the two cockpits;\n",
      "-----------------\n",
      "the rear fuselage was raised at the start of a landing descent to increase drag and slow the aircraft.\n",
      "tensor([ 0.0167, -0.1084, -0.0700,  ..., -0.0494, -0.0391,  0.0115],\n",
      "       device='cuda:0')\n",
      "The rear fuselage was raised at the start of a descent to slow the aircraft and increase drag.\n",
      "-----------------\n",
      "Early flight trials, with H.J.Pain as pilot revealed a need to stiffen the engine mountings.\n",
      "tensor([-0.0393,  0.0401, -0.0317,  ..., -0.0269, -0.0402,  0.0294],\n",
      "       device='cuda:0')\n",
      "Early flight trials, with H.J.P. Paine, revealed a need to stiffen the engine mountings.\n",
      "-----------------\n",
      "When this was done, the Vagabond, now fitted with a three-cylinder 1,095 cc Blackburne Thrush radial engine flew well enough at Lympne, but was eliminated in the preliminary rounds.\n",
      "tensor([-0.0715, -0.0310, -0.1932,  ..., -0.0179,  0.0826,  0.1376],\n",
      "       device='cuda:0')\n",
      "At this time, the Vagabond was fitted with a new 1,095 cc three-cylinder Lynx Thrush radial engine, now based in Blenheim, but was run poorly and failed to qualify in the preliminary rounds.\n",
      "-----------------\n",
      "Only one Vagabond, registered  as G-EBJF on 1 July 1924 was built.\n",
      "tensor([ 0.0666, -0.0250, -0.1873,  ..., -0.0577,  0.0478,  0.1484],\n",
      "       device='cuda:0')\n",
      "A only Vagabond, registered G-EBJF, was built on 19 July 1924.\n",
      "-----------------\n",
      "It was deregistered on 24 January 1928.\n",
      "tensor([-0.0755, -0.1108, -0.0881,  ...,  0.0449,  0.0314,  0.0690],\n",
      "       device='cuda:0')\n",
      "It was deregistered on 24 January 1928.\n",
      "-----------------\n",
      "Specifications\n",
      "tensor([ 0.0180, -0.0330, -0.1140,  ..., -0.0272,  0.0659,  0.0015],\n",
      "       device='cuda:0')\n",
      "Specifications\n",
      "-----------------\n",
      "References\n",
      "tensor([-0.0647, -0.0911, -0.0999,  ...,  0.0145,  0.0284,  0.0595],\n",
      "       device='cuda:0')\n",
      "References\n",
      "-----------------\n",
      "Notes\n",
      "tensor([-0.1633, -0.1352, -0.1033,  ...,  0.0108,  0.0029,  0.0855],\n",
      "       device='cuda:0')\n",
      "Notes\n",
      "-----------------\n",
      "Bibliography\n",
      "tensor([-0.1059, -0.0513, -0.1147,  ...,  0.0108,  0.0249,  0.0913],\n",
      "       device='cuda:0')\n",
      "Bibliography\n",
      "-----------------\n",
      "1920s British sport aircraft\n",
      "Vagabond\n",
      "Biplanes\n",
      "Single-engined tractor aircraft\n",
      "Aircraft first flown in 1924\n",
      "tensor([ 0.0174,  0.0672, -0.0563,  ..., -0.0035, -0.0178, -0.0068],\n",
      "       device='cuda:0')\n",
      "1920s British sport aircraft Vagabond Biplane Single-engined tractor aircraft First Aircraft flown in 1924\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(test_wiki_embeddings.column_names)\n",
    "for e in test_wiki_embeddings:\n",
    "    for i in range(len(e['sentences'])):\n",
    "        print(e['sentences'][i])\n",
    "        embeddings = torch.tensor(e['embeddings'][i]).to(device)  # Move embeddings to the same device\n",
    "        print(embeddings)\n",
    "        print(autoencoder.generate_from_latent(embeddings))\n",
    "        print('-----------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the embeddings and read them back to prove that we have a working representation of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1365.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the test_wiki_embeddings dataset to disk\n",
    "test_wiki_embeddings.save_to_disk('test_wiki_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset back from disk\n",
    "test_wiki_embeddings_back = Dataset.load_from_disk('test_wiki_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'url', 'title', 'text', 'sentences', 'embeddings']\n",
      "The Vickers Vagabond was Vickers' entrant for the second Lympne light aircraft competition, held in 1924.\n",
      "tensor([ 0.0198,  0.0262, -0.0087,  ..., -0.0770,  0.0140,  0.1913],\n",
      "       device='cuda:0')\n",
      "The Vickers Vagabond was a Vickers' entry for the second Lympne light aircraft competition, held in 1924.\n",
      "-----------------\n",
      "It was a conventional small biplane, with a very unusual method of trimming.\n",
      "tensor([-0.1479,  0.0048, -0.0186,  ...,  0.0602,  0.0649, -0.0328],\n",
      "       device='cuda:0')\n",
      "It was a conventional small biplane, with a very unusual method of trimming.\n",
      "-----------------\n",
      "It was eliminated from the trials at an early stage and only one was built.\n",
      "tensor([-0.1034,  0.0481, -0.0100,  ..., -0.0298,  0.0395, -0.0027],\n",
      "       device='cuda:0')\n",
      "It was eliminated from the trials at an early stage and only one was built.\n",
      "-----------------\n",
      "Development\n",
      "Following the first Lympne trials held in 1923 for single-seat motor-gliders, the Air Ministry organised a similar event in 1924, this time for low-powered two-seat aircraft.\n",
      "tensor([ 0.0045, -0.0924, -0.0048,  ..., -0.0252, -0.0517,  0.1219],\n",
      "       device='cuda:0')\n",
      "Following the first Lympne Motor Trials held in 1923 for single-seat flying-boats, the Ministry of Aviation organised a second event in 1924, this time for small two-seater airliners.\n",
      "-----------------\n",
      "The engine capacity limit was set at 1,100 cc.\n",
      "tensor([-0.0319, -0.0102,  0.0683,  ..., -0.0054,  0.0077,  0.0113],\n",
      "       device='cuda:0')\n",
      "The engine capacity limit was set at 1,100 cc.\n",
      "-----------------\n",
      "and, as before, the wings had to fold for easy transport and storage.\n",
      "tensor([-0.0499, -0.0388, -0.0723,  ..., -0.0093,  0.0026, -0.0202],\n",
      "       device='cuda:0')\n",
      "The wings and, as before, the wheels had to fold up for easy transport and storage.\n",
      "-----------------\n",
      "The trials took place between 29 September and 4 October.\n",
      "tensor([-0.0766, -0.0473,  0.0771,  ..., -0.0193,  0.0845, -0.0416],\n",
      "       device='cuda:0')\n",
      "The trials took place between 29 September and 4 October.\n",
      "-----------------\n",
      "Several companies built aircraft for them, including the Blackburn Bluebird, Hawker Cygnet, Supermarine Sparrow and two from Westland, the Woodpigeon and Widgeon.\n",
      "tensor([ 0.0248, -0.0528,  0.1389,  ...,  0.0011, -0.0074, -0.0192],\n",
      "       device='cuda:0')\n",
      "Several companies built aircraft for them, including the Blackburn Bluebird, Hawker Siddeley Cygnet, Supermarine Spitfire and two from Westland, the Woody Islander and Wigwam.\n",
      "-----------------\n",
      "The Type 98 Vagabond was Vickers' entry.\n",
      "tensor([ 0.0463, -0.0443,  0.0181,  ...,  0.0169,  0.1416,  0.0895],\n",
      "       device='cuda:0')\n",
      "The Type 98 Vagabond was Vickers' entry.\n",
      "-----------------\n",
      "It was a single-bay, wire-braced biplane with wings of constant chord except towards the rounded trailing tips.\n",
      "tensor([-0.0332,  0.0526, -0.0985,  ...,  0.1479, -0.0814,  0.0954],\n",
      "       device='cuda:0')\n",
      "It was a single-bay, wire-braced biplane with constant chord except towards the tips of rounded trailing edges.\n",
      "-----------------\n",
      "The wings had equal span and carried marked stagger.\n",
      "tensor([ 0.0359, -0.1104,  0.0220,  ...,  0.0670, -0.1214,  0.0330],\n",
      "       device='cuda:0')\n",
      "The wings had equal span and carried marked stagger.\n",
      "-----------------\n",
      "There were ailerons on both upper and lower wings, with flaps inboard on the lower wings which could be folded to assist wing-folding.\n",
      "tensor([ 0.0362, -0.0381,  0.0053,  ..., -0.0139,  0.0284, -0.0410],\n",
      "       device='cuda:0')\n",
      "There were ailerons on both upper and lower wings, with flaps inboard which could be folded to assist in wing-folding.\n",
      "-----------------\n",
      "The pilot and passenger sat in open cockpits, the latter under the upper wing.\n",
      "tensor([-0.0001, -0.0096, -0.0027,  ..., -0.0647, -0.0210, -0.0060],\n",
      "       device='cuda:0')\n",
      "The pilot and passenger sat in open cockpits, the latter under the upper wing.\n",
      "-----------------\n",
      "The pilot's upward view was enhanced by a small cutout in the trailing edge of the top wing.\n",
      "tensor([ 0.0193, -0.0109,  0.0042,  ...,  0.0455, -0.1016,  0.0169],\n",
      "       device='cuda:0')\n",
      "The pilot's upward view was enhanced by a cutout in the trailing edge of the small wing.\n",
      "-----------------\n",
      "The fuselage had a more rounded cross-section than that of the earlier Viget, Vickers' single-seat entry to the 1923 competition, extending a little below the lower wing.\n",
      "tensor([ 0.0170, -0.0488, -0.0571,  ...,  0.0007, -0.0761,  0.0821],\n",
      "       device='cuda:0')\n",
      "The fuselage had a more rounded cross-section than the earlier Vickers, the first member of the Gieget's single-seat entry to the 1923 competition, extending a little below the wing.\n",
      "-----------------\n",
      "The 32 hp (24 kW)  Bristol Cherub III flat twin engine was mounted in a smooth nose with the finned cylinders exposed for air cooling.\n",
      "tensor([ 0.0691,  0.0517, -0.0773,  ...,  0.0760, -0.0616,  0.0976],\n",
      "       device='cuda:0')\n",
      "The Bristol Cherub III engine had a 32 hp (24 kW) flat six engine mounted in the nose with the swept fins exposed for air cooling.\n",
      "-----------------\n",
      "The horizontal tail was similar to that of the Viget, but the fin and rudder were much more rounded.\n",
      "tensor([-0.1595, -0.1435,  0.0014,  ...,  0.0150, -0.0537,  0.0514],\n",
      "       device='cuda:0')\n",
      "The horizontal tail was similar to the Viget, but the fin and rudder were much more rounded.\n",
      "-----------------\n",
      "Because of the stagger, the mainwheels were in front of the lower wing, braced to the lower fuselage logeron aft to the front wing spar and forward to a point roughly below the upper wing leading edge.\n",
      "tensor([-0.0244, -0.1334,  0.0395,  ..., -0.0652, -0.0602,  0.0694],\n",
      "       device='cuda:0')\n",
      "Because of the high wing, the mainsails were staggered to the rear of the lower fuselage, braced to the lower wing spar and aft to the forward ailerons point and under a wing leaflet a little forward of the leading edge nearer to the lower fin.\n",
      "-----------------\n",
      "A most unusual feature of the Vagabond was the method of longitudinal trimming.\n",
      "tensor([-0.1214,  0.0206,  0.0215,  ...,  0.0878,  0.1041,  0.0356],\n",
      "       device='cuda:0')\n",
      "A most unusual feature of the Vagabond was the method of longitudinal trimming.\n",
      "-----------------\n",
      "Rather than changing the angle of the tailplane with respect to the fuselage, the whole rear part of the fuselage was hinged just ahead of the lower wing's trailing edge.\n",
      "tensor([-0.0336, -0.0696, -0.0594,  ..., -0.1674, -0.0283,  0.0271],\n",
      "       device='cuda:0')\n",
      "Rather than changing the angle of the tailplane with respect to the fuselage, the whole rear fuselage was hinged up just ahead of the lower part of the wing's trailing edge.\n",
      "-----------------\n",
      "This was controlled via a handwheel between the two cockpits;\n",
      "tensor([-0.0710, -0.0612, -0.0160,  ..., -0.0893, -0.0306, -0.0169],\n",
      "       device='cuda:0')\n",
      "This was controlled via a handwheel between the two cockpits;\n",
      "-----------------\n",
      "the rear fuselage was raised at the start of a landing descent to increase drag and slow the aircraft.\n",
      "tensor([ 0.0250, -0.1015, -0.1114,  ..., -0.0408, -0.0332,  0.0300],\n",
      "       device='cuda:0')\n",
      "the rear fuselage was raised at the start of a descent to slow the aircraft and increase drag.\n",
      "-----------------\n",
      "Early flight trials, with H.J.Pain as pilot revealed a need to stiffen the engine mountings.\n",
      "tensor([-0.0349,  0.0376, -0.0341,  ..., -0.0340, -0.0364,  0.0240],\n",
      "       device='cuda:0')\n",
      "Early flight trials, with H.J.Painty, revealed a need to stiffen the engine mountings.\n",
      "-----------------\n",
      "When this was done, the Vagabond, now fitted with a three-cylinder 1,095 cc Blackburne Thrush radial engine flew well enough at Lympne, but was eliminated in the preliminary rounds.\n",
      "tensor([-0.0709, -0.0319, -0.1934,  ..., -0.0208,  0.0839,  0.1348],\n",
      "       device='cuda:0')\n",
      "At this point the Valiant, now fitted with a three-cylinder 1,095 cc Vulcan Lysander engine, was flown in the Bungalow Cup round at Blackburn, but was unsuccessful, but it cleared the preliminary floats.\n",
      "-----------------\n",
      "Only one Vagabond, registered  as G-EBJF on 1 July 1924 was built.\n",
      "tensor([ 0.0941, -0.0189, -0.1780,  ..., -0.0755,  0.0478,  0.1232],\n",
      "       device='cuda:0')\n",
      "One Vagabond, registered G-EBJF, was built on 19 July 1924.\n",
      "-----------------\n",
      "It was deregistered on 24 January 1928.\n",
      "tensor([-0.0594, -0.1222, -0.0821,  ..., -0.0114,  0.0506,  0.0607],\n",
      "       device='cuda:0')\n",
      "It was deregistered on 24 January 1928.\n",
      "-----------------\n",
      "Specifications\n",
      "tensor([ 0.0710,  0.0265, -0.0482,  ..., -0.0622,  0.0740, -0.0381],\n",
      "       device='cuda:0')\n",
      "Specifications\n",
      "-----------------\n",
      "References\n",
      "tensor([-0.0155, -0.0479, -0.0648,  ..., -0.0377,  0.0588, -0.0006],\n",
      "       device='cuda:0')\n",
      "References\n",
      "-----------------\n",
      "Notes\n",
      "tensor([-0.1813, -0.1582, -0.0936,  ..., -0.0950,  0.0393,  0.0871],\n",
      "       device='cuda:0')\n",
      "Notes\n",
      "-----------------\n",
      "Bibliography\n",
      "tensor([-0.0842,  0.0068, -0.0746,  ..., -0.0150,  0.0215,  0.0626],\n",
      "       device='cuda:0')\n",
      "Bibliography\n",
      "-----------------\n",
      "1920s British sport aircraft\n",
      "Vagabond\n",
      "Biplanes\n",
      "Single-engined tractor aircraft\n",
      "Aircraft first flown in 1924\n",
      "tensor([ 0.0351,  0.0666, -0.0565,  ..., -0.0154, -0.0174, -0.0032],\n",
      "       device='cuda:0')\n",
      "1920s British sport aircraft Vagabond Biplane Single-engine tractor aircraft First Aircraft flown in 1924\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(test_wiki_embeddings_back.column_names)\n",
    "for e in test_wiki_embeddings_back:\n",
    "    for i in range(len(e['sentences'])):\n",
    "        print(e['sentences'][i])\n",
    "        embeddings = torch.tensor(e['embeddings'][i]).to(device)  # Move embeddings to the same device\n",
    "        print(embeddings)\n",
    "        print(autoencoder.generate_from_latent(embeddings))\n",
    "        print('-----------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - now lets do it for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6775235\n",
      "['id', 'url', 'title', 'text']\n"
     ]
    }
   ],
   "source": [
    "full_wiki = wiki['train']\n",
    "print(len(full_wiki))\n",
    "print(full_wiki.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = full_wiki.map(lambda x: {'sentences': get_sentences(x['text'])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   3%|▎         | 171934/6775235 [7:36:36<292:16:28,  6.28 examples/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wiki_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mwiki_sentences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_embeddings_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wiki_embeddings\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwiki_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/datasets/arrow_dataset.py:3093\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3088\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3089\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3090\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3091\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3092\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3093\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3094\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3095\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/datasets/arrow_dataset.py:3446\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3444\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3446\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3448\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/datasets/arrow_dataset.py:3349\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3348\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3349\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3351\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3352\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3353\u001b[0m     }\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wiki_embeddings \u001b[38;5;241m=\u001b[39m wiki_sentences\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mget_embeddings_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m})\n\u001b[1;32m      2\u001b[0m wiki_embeddings\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwiki_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mget_embeddings_batch\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings_batch\u001b[39m(sentences):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mBottleneckT5Autoencoder.embed\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     28\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(batch, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     29\u001b[0m     decoder_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 30\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencode_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/thesephist/contra-bottleneck-t5-large-wikipedia/6eedf53ea877e4a94044d3866ffb54a3339284ba/bottleneck_t5.py:320\u001b[0m, in \u001b[0;36mBottleneckT5LMWithPerturb.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, perturb_vector, encode_only)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m    330\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m    331\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    332\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    334\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1110\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1096\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1097\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         output_attentions,\n\u001b[1;32m   1108\u001b[0m     )\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:754\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    751\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:342\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 342\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/TransformerAEC/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:254\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 254\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wiki_embeddings = wiki_sentences.map(lambda x: {'embeddings': get_embeddings_batch(x['sentences'])})\n",
    "wiki_embeddings.save_to_disk('wiki_embeddings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "10\n",
      "10\n",
      "352\n",
      "8\n",
      "18\n",
      "36\n",
      "19\n",
      "22\n",
      "8\n",
      "17\n",
      "4\n",
      "29\n",
      "35\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for i in range(3985, 4000):\n",
    "    b = wiki_sentences[i]['sentences']\n",
    "    print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 sentences with 5910 words.\n"
     ]
    }
   ],
   "source": [
    "b = wiki_sentences[3988]['sentences']\n",
    "length = sum([len(s.split()) for s in b])\n",
    "print(f'{len(b)} sentences with {length} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attempt has been abandoned because the amount of time required to convert wikipedia to embeddings was excessive. (3% took 7 hours on my RTX3090, which was running at 92% utilisation)\n",
    "\n",
    "I have switched my attention to exploring how well the AEC performs on different tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformerAEC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
