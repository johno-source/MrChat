{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Agents with Langraph and Ollama\n",
    "The purpose of this notebook is to look at the issues associated with getting agents to work locally. Some videos on langraph have convinced me that it may be a more\n",
    "flexible way to address the implementation of my own agentic architectures. That is explored in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying final output format\n",
    "from IPython.display import display, Markdown, Latex\n",
    "# LangChain Dependencies\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# For State Graph \n",
    "from typing_extensions import TypedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Ollama Agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LLM\n",
    "local_llm = 'phi3:14b-medium-4k-instruct-q8_0'\n",
    "model = ChatOllama(model=local_llm, temperature=0, base_url=\"http://192.168.86.2:11434\", keep_alive=-1)\n",
    "model_json = ChatOllama(model=local_llm, format='json', temperature=0, base_url=\"http://192.168.86.2:11434\", keep_alive=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search Tool\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=25)\n",
    "web_search_tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "\n",
    "# Test Run\n",
    "# resp = web_search_tool.invoke(\"home depot news\")\n",
    "# resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choice': 'generate'}\n"
     ]
    }
   ],
   "source": [
    "# Router\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [(\"system\", \"\"\"\n",
    "    You are an expert at routing a user question to either the generation stage or web search. \n",
    "    Use the web search for questions that require more context for a better answer, or recent events.\n",
    "    Otherwise, you can skip and go straight to the generation phase to respond.\n",
    "    You do not need to be stringent with the keywords in the question related to these topics.\n",
    "    Give a binary choice 'web_search' or 'generate' based on the question. \n",
    "    Return the JSON with a single key 'choice' with no premable or explanation. \n",
    "    \n",
    "    Question to route: {question} \n",
    "    \"\"\"\n",
    "    ),]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "question_router = router_prompt | model_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "question = \"What's up?\"\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n\\n{ \"choice\": \"generate\" }\\n\\n```' response_metadata={'model': 'phi3:14b-medium-4k-instruct-q8_0', 'created_at': '2024-05-24T07:54:55.370119971Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 442952241, 'load_duration': 2046245, 'prompt_eval_count': 11, 'prompt_eval_duration': 117445000, 'eval_count': 16, 'eval_duration': 319574000} id='run-e2d99d64-948d-4edd-b259-8cfd54c9c8d5-0'\n"
     ]
    }
   ],
   "source": [
    "# Chain\n",
    "question_router = router_prompt | model\n",
    "\n",
    "# Test Run\n",
    "question = \"What's up?\"\n",
    "response = question_router.invoke(\"whats up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(response.response_metadata['prompt_eval_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content=\"\\n    You are an expert at routing a user question to either the generation stage or web search. \\n    Use the web search for questions that require more context for a better answer, or recent events.\\n    Otherwise, you can skip and go straight to the generation phase to respond.\\n    You do not need to be stringent with the keywords in the question related to these topics.\\n    Give a binary choice 'web_search' or 'generate' based on the question. \\n    Return the JSON with a single key 'choice' with no premable or explanation. \\n    \\n    Question to route: Whats up? \\n    \")]\n"
     ]
    }
   ],
   "source": [
    "print(router_prompt.invoke(\"Whats up?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content=\"\\n        You are an AI assistant for Research Question Tasks, that synthesizes web search results. \\n        Strictly use the following pieces of web search context to answer the question. If you don't know the answer, just say that you don't know. \\n        keep the answer concise, but provide all of the details you can in the form of a research report. \\n        Only make direct references to material if provided in the context.\"), HumanMessage(content=\"\\n    Question: What's been up with Macom recently? \\n    Web Search Context:  \\n    Answer: \")]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_template = \"\"\"\n",
    "        You are an AI assistant for Research Question Tasks, that synthesizes web search results. \n",
    "        Strictly use the following pieces of web search context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "        keep the answer concise, but provide all of the details you can in the form of a research report. \n",
    "        Only make direct references to material if provided in the context.\"\"\"\n",
    "user_template = \"\"\"\n",
    "    Question: {question} \n",
    "    Web Search Context: {context} \n",
    "    Answer: \"\"\"\n",
    "\n",
    "generate_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('human', user_template),\n",
    "])\n",
    "\n",
    "generate_result = generate_prompt.invoke( {\n",
    "    \"question\": \"What's been up with Macom recently?\", \"context\": \"\" }\n",
    ")\n",
    "print(generate_result)\n",
    "\n",
    "# Chain\n",
    "generate_chain = generate_prompt | llama3 | StrOutputParser()\n",
    "\n",
    "# Test Run\n",
    "# question = \"How are you?\"\n",
    "# context = \"\"\n",
    "#generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "#print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama3_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m query_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m      4\u001b[0m     [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    You are an expert at crafting web search queries for research questions.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)]\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Chain\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m query_chain \u001b[38;5;241m=\u001b[39m query_prompt \u001b[38;5;241m|\u001b[39m \u001b[43mllama3_json\u001b[49m \u001b[38;5;241m|\u001b[39m JsonOutputParser()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Test Run\u001b[39;00m\n\u001b[1;32m     18\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms happened recently with Macom?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llama3_json' is not defined"
     ]
    }
   ],
   "source": [
    "# Query Transformation\n",
    "\n",
    "query_prompt = ChatPromptTemplate.from_messages(\n",
    "    ('system', \"\"\"\n",
    "    You are an expert at crafting web search queries for research questions.\n",
    "    More often than not, a user will ask a basic question that they wish to learn more about, however it might not be in the best format. \n",
    "    Reword their query to be the most effective web search string possible.\n",
    "    Return the JSON with a single key 'query' with no premable or explanation. \n",
    "    \n",
    "    Question to transform: {question} \n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Chain\n",
    "query_chain = query_prompt | llama3_json | JsonOutputParser()\n",
    "\n",
    "# Test Run\n",
    "question = \"What's happened recently with Macom?\"\n",
    "print(query_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_query: revised question for web search\n",
    "        context: web_search result\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    search_query : str\n",
    "    context : str\n",
    "\n",
    "# Node - Generate\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Node - Query Transformation\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform user question to web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended search query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Optimizing Query for Web Search\")\n",
    "    question = state['question']\n",
    "    gen_query = query_chain.invoke({\"question\": question})\n",
    "    search_query = gen_query[\"query\"]\n",
    "    return {\"search_query\": search_query}\n",
    "\n",
    "\n",
    "# Node - Web Search\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to context\n",
    "    \"\"\"\n",
    "\n",
    "    search_query = state['search_query']\n",
    "    print(f'Step: Searching the Web for: \"{search_query}\"')\n",
    "    \n",
    "    # Web search tool call\n",
    "    search_result = web_search_tool.invoke(search_query)\n",
    "    return {\"context\": search_result}\n",
    "\n",
    "\n",
    "# Conditional Edge, Routing\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    route question to web search or generation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Step: Routing Query\")\n",
    "    question = state['question']\n",
    "    output = question_router.invoke({\"question\": question})\n",
    "    if output['choice'] == \"web_search\":\n",
    "        print(\"Step: Routing Query to Web Search\")\n",
    "        return \"websearch\"\n",
    "    elif output['choice'] == 'generate':\n",
    "        print(\"Step: Routing Query to Generation\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the nodes\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Build the edges\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"websearch\")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "local_agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query):\n",
    "    output = local_agent.invoke({\"question\": query})\n",
    "    print(\"=======\")\n",
    "    display(Markdown(output[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Routing Query to Web Search\n",
      "Step: Optimizing Query for Web Search\n",
      "Step: Searching the Web for: \"Macom recent news updates\"\n",
      "Step: Generating Final Response\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the available web search context, there is no recent information about Macom to provide. Further research may be required using different sources or keywords for more updated details regarding Macom's activities and developments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test it out!\n",
    "run_agent(\"What's been up with Macom recently?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if ChatOllama uses the langchain messages and inserts the correct tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"When dawn doth break, and night's dark veil recedes,  \\nThe eastern skies blush pink in morning light;  \\nA canvas painted by Aurora's deeds,  \\nAs stars retreat from Apollo's rising might.\\n\\nAt noon the azure heavens stretch so wide,  \\nWith cotton clouds that drift on gentle breeze;  \\nThe sun at zenith in its lofty pride,  \\nBestows a warmth upon the earth with ease.\\n\\nAs day doth wane and shadows start to grow,  \\nA tapestry of orange hues takes flight;  \\nEvening's approach is marked by amber glow,  \\nAnd twilight whispers softly into night.\\n\\nIn darkness deep, the moon her vigil keeps,  \\nWhile stars above in silent splendor peep.\" response_metadata={'model': 'phi3:14b-medium-4k-instruct-q8_0', 'created_at': '2024-05-23T07:30:45.448378157Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 4263597286, 'load_duration': 1611359, 'prompt_eval_count': 36, 'prompt_eval_duration': 119083000, 'eval_count': 196, 'eval_duration': 4139918000} id='run-4cbe5b4d-f18e-479b-93e2-43b99ac5f1c7-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that always responds with a Shakespearean sonnet.\"),\n",
    "    HumanMessage(\n",
    "        content=\"What color is the sky at different times of the day?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "chat_model_response = llama3.invoke(messages)\n",
    "print(chat_model_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So that worked correctly. Lets rework the above prompts to use the standard prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now make a chatbot from phi3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([ \n",
    "    ('system', 'You are a helpful assistant.' ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    ('human', '{input}' ),\n",
    "])\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hi. My name is John. How are you?\")\n",
    "\n",
    "chat_chain = chat_prompt | llama3 | StrOutputParser()\n",
    "print(chat_chain.invoke({'input':\"Do you remember my name?\", 'chat_history':history.messages}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: Hi. My name is John. How are you?\n"
     ]
    }
   ],
   "source": [
    "for m in history.messages:\n",
    "    print(f'{m.type}: {m.content}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
