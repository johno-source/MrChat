{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is Groq?\n",
    "This file is to see if groq can be used and how expensive it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def fmt(str):\n",
    "    formatted_lines = [textwrap.fill(line, width=120) for line in str.split('\\n')]\n",
    "    return '\\n'.join(formatted_lines)\n",
    "\n",
    "def print_(str):\n",
    "    print(fmt(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I'd be happy to explain!\n",
      "\n",
      "LLM stands for \"Low Latency Logging,\" which is a method of recording and transmitting data with minimal delay. Low\n",
      "latency LLMs are particularly important in time-sensitive applications, such as financial trading, online gaming, and\n",
      "real-time data processing.\n",
      "\n",
      "Here are some reasons why low latency LLMs are important:\n",
      "\n",
      "1. Faster Decision Making: In time-sensitive applications, every millisecond counts. Low latency LLMs ensure that data\n",
      "is transmitted and processed quickly, allowing for faster decision making. For example, in high-frequency trading, low\n",
      "latency LLMs can help traders make split-second decisions that can result in significant financial gains.\n",
      "2. Improved User Experience: In online gaming and other real-time applications, low latency LLMs can help improve the\n",
      "user experience by reducing lag and ensuring smooth, responsive interactions. This is particularly important in\n",
      "multiplayer games, where even small delays can impact gameplay and lead to a poor user experience.\n",
      "3. Increased Efficiency: Low latency LLMs can help increase efficiency by reducing the time it takes to transmit and\n",
      "process data. This can lead to cost savings and improved productivity, particularly in industries where data processing\n",
      "is a critical component of the business.\n",
      "4. Enhanced Security: Low latency LLMs can also help enhance security by reducing the amount of time that data is\n",
      "vulnerable to attack. By transmitting data quickly and efficiently, low latency LLMs can help reduce the risk of data\n",
      "breaches and other security threats.\n",
      "\n",
      "Overall, low latency LLMs are an essential component of many time-sensitive applications. By ensuring fast, efficient,\n",
      "and secure data transmission, low latency LLMs can help businesses and individuals make better decisions, improve user\n",
      "experiences, increase efficiency, and enhance security.\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "print_(chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"}).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'gemma-7b-it', 'object': 'model', 'created': 1693721698, 'owned_by': 'Google', 'active': True, 'context_window': 8192}, {'id': 'llama3-70b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192}, {'id': 'llama3-8b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192}, {'id': 'mixtral-8x7b-32768', 'object': 'model', 'created': 1693721698, 'owned_by': 'Mistral AI', 'active': True, 'context_window': 32768}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json()['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq()\n",
    "\n",
    "def groq_prompt(prompt):\n",
    "    convo = [{'role': 'user', 'content': prompt}]\n",
    "    chat_completion = groq_client.chat.completions.create(messages=convo, model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why doesn't Donald Trump use bookmarks? He doesn't want people to think he's well-read.\n"
     ]
    }
   ],
   "source": [
    "# test the prompt\n",
    "prompt = input('USER: ')\n",
    "response = groq_prompt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageGrab\n",
    "\n",
    "def function_call(prompt):\n",
    "    sys_msg = (\n",
    "        'You are an AI function calling model. You will determine whether the users clipboard content, '\n",
    "        'taking a screenshot, capturing the webcam or calling no functions is best for a voice assistant to respond '\n",
    "        'to the users prompt. The webcam can be assumed to be a normal laptop webcam facing the user. You will '\n",
    "        'respond with only one selection from this list: [\"extract clipboard\", \"take screenshot\", \"capture webcam\", \"None\"] \\n'\n",
    "        'Do not respond with anything but the most logical selection from that list with no explanation. Format the '\n",
    "        'function call name exactly as I listed.'\n",
    "    ) \n",
    "\n",
    "    function_convo = [{'role': 'system', 'content': sys_msg},\n",
    "                      {'role': 'user', 'content': prompt}]\n",
    "    \n",
    "    chat_completion = groq_client.chat.completions.create(messages=function_convo, model=\"mixtral-8x7b-32768\")\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def take_screenshot():\n",
    "    path = 'screenshot.jpg'\n",
    "    screenshot = ImageGrab.grab()\n",
    "    rgb_screenshot = screenshot.convert('RGB')\n",
    "    rgb_screenshot.save(path, quality=15)\n",
    "\n",
    "def web_cam_capture():\n",
    "    return None\n",
    "\n",
    "def get_clipboard_text():\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract clipboard\n",
      "\"capture webcam\"\n"
     ]
    }
   ],
   "source": [
    "prompt = 'I want you to analyse the code I put on the clipboard.'\n",
    "response = function_call(prompt)\n",
    "print(response)\n",
    "print(function_call(\"I am holding amy dog up to the webcam. What is on his nose?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Camera did not open successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "web_cam = cv2.VideoCapture()\n",
    "\n",
    "def web_cam_capture():\n",
    "    if not web_cam.isOpened():\n",
    "        print('Error: Camera did not open successfully')\n",
    "        return\n",
    "    \n",
    "    path = 'webcam.jpg'\n",
    "    ret, frame = web_cam.read()\n",
    "    cv2.imwrite(path, frame)\n",
    "\n",
    "web_cam_capture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/MrChat/GroqChat\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
