{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Multiple Ollama instances\n",
    "There was a problem having two different gpus inside the one ollama image. I want to have more control so I can put my smaller models on my smaller gpu and save my big model on my 3090. This code tests the implememtation of a multple ollama server implementation. There are 2 docker images, one with each gpu. port 11434 has the 3090 and port 11433 has the 3070. Lets test how things go.\n",
    "\n",
    "First test the smaller gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question \"What is the meaning of life?\" has intrigued philosophers, theologians, scientists, and thinkers for\n",
      "centuries. There isn't a single answer that applies universally as perspectives on this vary widely based on cultural,\n",
      "religious, and individual beliefs. Here are some viewpoints:\n",
      "\n",
      "\n",
      "1. Philosophical Viewpoint: Many philosophers argue that life doesn't have an inherent meaning but rather it is up to\n",
      "each individual to create their own purpose through choices and actions.\n",
      "\n",
      "2. Religious Perspective: Different religions offer various answers, suggesting the purpose of life may be related to\n",
      "spiritual fulfillment or following a divine plan set by a higher power.\n",
      "\n",
      "3. Scientific Angle: From an evolutionary standpoint, one might argue that the meaning of life is simply to survive and\n",
      "reproduce, passing on genes through biological processes.\n",
      "\n",
      "4. Personal Fulfilment: Many believe in finding personal meaning through experiences, relationships, achievements, or a\n",
      "combination thereof.\n",
      "\n",
      "5. No Meaning at All: Some existentialist thinkers suggest that if life has no predetermined meaning, it's up to\n",
      "individuals to define their own existence.\n",
      "\n",
      "\n",
      "It is essential to understand that these are broad interpretations and the question itself might be more about exploring\n",
      "one's personal beliefs or values than seeking a definitive answer applicable to everyone.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def fmt(str):\n",
    "    formatted_lines = [textwrap.fill(line, width=120) for line in str.split('\\n')]\n",
    "    return '\\n'.join(formatted_lines)\n",
    "\n",
    "\n",
    "\n",
    "llm = Ollama(base_url=\"http://192.168.86.2:11433\", model=\"phi3\", request_timeout=360.0)\n",
    "\n",
    "print(fmt(str(llm.complete(\"What is the meaning of life?\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now show the other gpu running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The age-old question!\n",
      "\n",
      "The meaning of life is a topic that has been debated and explored by philosophers, theologians, scientists, and many\n",
      "others for centuries. There is no one definitive answer, as it is a highly subjective and personal question that can\n",
      "vary greatly from person to person.\n",
      "\n",
      "Here are some possible perspectives on the meaning of life:\n",
      "\n",
      "1. **Biological perspective**: From a biological standpoint, the meaning of life might be to survive, reproduce, and\n",
      "pass on genetic information to future generations.\n",
      "2. **Philosophical perspective**: Philosophers have offered various answers, such as:\n",
      "        * To seek happiness or fulfillment (eudaimonia).\n",
      "        * To pursue knowledge, wisdom, and personal growth.\n",
      "        * To develop a sense of purpose, direction, or meaning in one's life.\n",
      "        * To find significance, value, or importance in the world.\n",
      "3. **Religious perspective**: Many religions believe that the meaning of life is to:\n",
      "        * Worship and serve a higher power (e.g., God).\n",
      "        * Follow a moral code or set of principles.\n",
      "        * Achieve spiritual enlightenment or salvation.\n",
      "4. **Psychological perspective**: From a psychological standpoint, the meaning of life might be tied to:\n",
      "        * Self-actualization: realizing one's full potential and living authentically.\n",
      "        * Finding purpose and direction in life.\n",
      "        * Developing positive relationships with others.\n",
      "5. **Existentialist perspective**: Existentialists believe that the meaning of life is created by individuals\n",
      "themselves, through their choices, actions, and experiences.\n",
      "6. **Scientific perspective**: Some scientists argue that the meaning of life might be an emergent property of complex\n",
      "systems, such as human societies or ecosystems.\n",
      "\n",
      "Ultimately, the meaning of life is a deeply personal question that each individual must answer for themselves. It may\n",
      "involve seeking answers to questions like:\n",
      "\n",
      "* What gives my life purpose and direction?\n",
      "* What values do I hold dear?\n",
      "* What kind of relationships do I want to have with others?\n",
      "* How can I make a positive impact on the world?\n",
      "\n",
      "Remember, there is no one \"right\" answer to this question. The meaning of life is a journey, not a destination, and it\n",
      "may evolve over time as we grow, learn, and experience new things.\n"
     ]
    }
   ],
   "source": [
    "llm_big_gpu = Ollama(base_url=\"http://192.168.86.2:11434\", model=\"llama3:8b-instruct-fp16\", request_timeout=360.0, keep_alive=-1)\n",
    "\n",
    "print(fmt(str(llm_big_gpu.complete(\"What is the meaning of life?\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Ollama in module llama_index.llms.ollama.base:\n",
      "\n",
      "class Ollama(llama_index.core.llms.custom.CustomLLM)\n",
      " |  Ollama(*, callback_manager: llama_index.core.callbacks.base.CallbackManager = None, system_prompt: Optional[str] = None, messages_to_prompt: Callable = None, completion_to_prompt: Callable = None, output_parser: Optional[llama_index.core.types.BaseOutputParser] = None, pydantic_program_mode: llama_index.core.types.PydanticProgramMode = <PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt: Optional[llama_index.core.prompts.base.BasePromptTemplate] = None, base_url: str = 'http://localhost:11434', model: str, temperature: float = 0.75, context_window: pydantic.v1.types.ConstrainedIntValue = 3900, request_timeout: float = 30.0, prompt_key: str = 'prompt', json_mode: bool = False, additional_kwargs: Dict[str, Any] = None) -> None\n",
      " |\n",
      " |  Ollama LLM.\n",
      " |\n",
      " |  Visit https://ollama.com/ to download and install Ollama.\n",
      " |\n",
      " |  Run `ollama serve` to start a server.\n",
      " |\n",
      " |  Run `ollama pull <name>` to download a model to run.\n",
      " |\n",
      " |  Examples:\n",
      " |      `pip install llama-index-llms-ollama`\n",
      " |\n",
      " |      ```python\n",
      " |      from llama_index.llms.ollama import Ollama\n",
      " |\n",
      " |      llm = Ollama(model=\"llama2\", request_timeout=60.0)\n",
      " |\n",
      " |      response = llm.complete(\"What is the capital of France?\")\n",
      " |      print(response)\n",
      " |      ```\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Ollama\n",
      " |      llama_index.core.llms.custom.CustomLLM\n",
      " |      llama_index.core.llms.llm.LLM\n",
      " |      llama_index.core.base.llms.base.BaseLLM\n",
      " |      llama_index.core.base.query_pipeline.query.ChainableMixin\n",
      " |      abc.ABC\n",
      " |      llama_index.core.schema.BaseComponent\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  async achat = wrapped_async_llm_chat(_self: Any, messages: Sequence[llama_index.core.base.llms.types.ChatMessage], **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_chat_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  async acomplete = wrapped_async_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_completion_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  async astream_complete = wrapped_async_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_completion_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  chat = wrapped_llm_chat(_self: Any, messages: Sequence[llama_index.core.base.llms.types.ChatMessage], **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_chat_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  complete = wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_completion_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  stream_chat = wrapped_llm_chat(_self: Any, messages: Sequence[llama_index.core.base.llms.types.ChatMessage], **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_chat_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  stream_complete = wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_completion_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  class_name() -> str\n",
      " |      Get the class name, used as a unique ID in serialization.\n",
      " |\n",
      " |      This provides a key that makes serialization robust against actual class\n",
      " |      name changes.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any from pydantic.v1.json\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  metadata\n",
      " |      LLM metadata.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'additional_kwargs': typing.Dict[str, typing.Any], ...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |\n",
      " |  __custom_root_type__ = False\n",
      " |\n",
      " |  __exclude_fields__ = {'callback_manager': True, 'completion_to_prompt'...\n",
      " |\n",
      " |  __fields__ = {'additional_kwargs': ModelField(name='additional_kwargs'...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __include_fields__ = None\n",
      " |\n",
      " |  __post_root_validators__ = [(False, <function LLM.check_prompts>)]\n",
      " |\n",
      " |  __pre_root_validators__ = []\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __schema_cache__ = {}\n",
      " |\n",
      " |  __signature__ = <Signature (*, callback_manager: llama_index.cor...ddi...\n",
      " |\n",
      " |  __validators__ = {'callback_manager': [<pydantic.v1.class_validators.V...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.llms.custom.CustomLLM:\n",
      " |\n",
      " |  async astream_chat = wrapped_async_llm_chat(_self: Any, messages: Sequence[llama_index.core.base.llms.types.ChatMessage], **kwargs: Any) -> Any from llama_index.core.llms.callbacks.llm_chat_callback.<locals>.wrap.<locals>\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.llms.llm.LLM:\n",
      " |\n",
      " |  async apredict(self, prompt: llama_index.core.prompts.base.BasePromptTemplate, **prompt_args: Any) -> str\n",
      " |      Async Predict for a given prompt.\n",
      " |\n",
      " |      Args:\n",
      " |          prompt (BasePromptTemplate):\n",
      " |              The prompt to use for prediction.\n",
      " |          prompt_args (Any):\n",
      " |              Additional arguments to format the prompt with.\n",
      " |\n",
      " |      Returns:\n",
      " |          str: The prediction output.\n",
      " |\n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          from llama_index.core.prompts import PromptTemplate\n",
      " |\n",
      " |          prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n",
      " |          output = await llm.apredict(prompt, topic=\"cats\")\n",
      " |          print(output)\n",
      " |          ```\n",
      " |\n",
      " |  async apredict_and_call(self, tools: List[ForwardRef('BaseTool')], user_msg: Union[str, llama_index.core.base.llms.types.ChatMessage, NoneType] = None, chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = None, verbose: bool = False, **kwargs: Any) -> 'AgentChatResponse'\n",
      " |      Predict and call the tool.\n",
      " |\n",
      " |  async astream(self, prompt: llama_index.core.prompts.base.BasePromptTemplate, **prompt_args: Any) -> AsyncGenerator[str, NoneType]\n",
      " |      Async stream predict for a given prompt.\n",
      " |\n",
      " |      Args:\n",
      " |      prompt (BasePromptTemplate):\n",
      " |          The prompt to use for prediction.\n",
      " |      prompt_args (Any):\n",
      " |          Additional arguments to format the prompt with.\n",
      " |\n",
      " |      Yields:\n",
      " |          str: An async generator that yields strings of tokens.\n",
      " |\n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          from llama_index.core.prompts import PromptTemplate\n",
      " |\n",
      " |          prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n",
      " |          gen = await llm.astream_predict(prompt, topic=\"cats\")\n",
      " |          async for token in gen:\n",
      " |              print(token, end=\"\", flush=True)\n",
      " |          ```\n",
      " |\n",
      " |  async astructured_predict(self, output_cls: pydantic.v1.main.BaseModel, prompt: llama_index.core.prompts.base.PromptTemplate, **prompt_args: Any) -> pydantic.v1.main.BaseModel\n",
      " |      Async Structured predict.\n",
      " |\n",
      " |      Args:\n",
      " |          output_cls (BaseModel):\n",
      " |              Output class to use for structured prediction.\n",
      " |          prompt (PromptTemplate):\n",
      " |              Prompt template to use for structured prediction.\n",
      " |          prompt_args (Any):\n",
      " |              Additional arguments to format the prompt with.\n",
      " |\n",
      " |      Returns:\n",
      " |          BaseModel: The structured prediction output.\n",
      " |\n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          from pydantic.v1 import BaseModel\n",
      " |\n",
      " |          class Test(BaseModel):\n",
      " |              \\\"\\\"\\\"My test class.\\\"\\\"\\\"\n",
      " |              name: str\n",
      " |\n",
      " |          from llama_index.core.prompts import PromptTemplate\n",
      " |\n",
      " |          prompt = PromptTemplate(\"Please predict a Test with a random name related to {topic}.\")\n",
      " |          output = await llm.astructured_predict(Test, prompt, topic=\"cats\")\n",
      " |          print(output.name)\n",
      " |          ```\n",
      " |\n",
      " |  predict(self, prompt: llama_index.core.prompts.base.BasePromptTemplate, **prompt_args: Any) -> str\n",
      " |      Predict for a given prompt.\n",
      " |\n",
      " |      Args:\n",
      " |          prompt (BasePromptTemplate):\n",
      " |              The prompt to use for prediction.\n",
      " |          prompt_args (Any):\n",
      " |              Additional arguments to format the prompt with.\n",
      " |\n",
      " |      Returns:\n",
      " |          str: The prediction output.\n",
      " |\n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          from llama_index.core.prompts import PromptTemplate\n",
      " |\n",
      " |          prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n",
      " |          output = llm.predict(prompt, topic=\"cats\")\n",
      " |          print(output)\n",
      " |          ```\n",
      " |\n",
      " |  predict_and_call(self, tools: List[ForwardRef('BaseTool')], user_msg: Union[str, llama_index.core.base.llms.types.ChatMessage, NoneType] = None, chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = None, verbose: bool = False, **kwargs: Any) -> 'AgentChatResponse'\n",
      " |      Predict and call the tool.\n",
      " |\n",
      " |      By default uses a ReAct agent to do tool calling (through text prompting),\n",
      " |      but function calling LLMs will implement this differently.\n",
      " |\n",
      " |  stream(self, prompt: llama_index.core.prompts.base.BasePromptTemplate, **prompt_args: Any) -> Generator[str, NoneType, NoneType]\n",
      " |      Stream predict for a given prompt.\n",
      " |\n",
      " |      Args:\n",
      " |          prompt (BasePromptTemplate):\n",
      " |              The prompt to use for prediction.\n",
      " |          prompt_args (Any):\n",
      " |              Additional arguments to format the prompt with.\n",
      " |\n",
      " |      Yields:\n",
      " |          str: Each streamed token.\n",
      " |\n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          from llama_index.core.prompts import PromptTemplate\n",
      " |\n",
      " |          prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n",
      " |          gen = llm.stream_predict(prompt, topic=\"cats\")\n",
      " |          for token in gen:\n",
      " |              print(token, end=\"\", flush=True)\n",
      " |          ```\n",
      " |\n",
      " |  structured_predict(self, output_cls: pydantic.v1.main.BaseModel, prompt: llama_index.core.prompts.base.PromptTemplate, **prompt_args: Any) -> pydantic.v1.main.BaseModel\n",
      " |      Structured predict.\n",
      " |\n",
      " |      Args:\n",
      " |          output_cls (BaseModel):\n",
      " |              Output class to use for structured prediction.\n",
      " |          prompt (PromptTemplate):\n",
      " |              Prompt template to use for structured prediction.\n",
      " |          prompt_args (Any):\n",
      " |              Additional arguments to format the prompt with.\n",
      " |\n",
      " |      Returns:\n",
      " |          BaseModel: The structured prediction output.\n",
      " |\n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          from pydantic.v1 import BaseModel\n",
      " |\n",
      " |          class Test(BaseModel):\n",
      " |              \\\"\\\"\\\"My test class.\\\"\\\"\\\"\n",
      " |              name: str\n",
      " |\n",
      " |          from llama_index.core.prompts import PromptTemplate\n",
      " |\n",
      " |          prompt = PromptTemplate(\"Please predict a Test with a random name related to {topic}.\")\n",
      " |          output = llm.structured_predict(Test, prompt, topic=\"cats\")\n",
      " |          print(output.name)\n",
      " |          ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from llama_index.core.llms.llm.LLM:\n",
      " |\n",
      " |  check_prompts(values: Dict[str, Any]) -> Dict[str, Any]\n",
      " |\n",
      " |  set_completion_to_prompt(completion_to_prompt: Optional[llama_index.core.llms.llm.CompletionToPromptType]) -> llama_index.core.llms.llm.CompletionToPromptType\n",
      " |\n",
      " |  set_messages_to_prompt(messages_to_prompt: Optional[llama_index.core.llms.llm.MessagesToPromptType]) -> llama_index.core.llms.llm.MessagesToPromptType\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.core.base.llms.base.BaseLLM:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from llama_index.core.base.llms.base.BaseLLM:\n",
      " |\n",
      " |  Config = <class 'llama_index.core.base.llms.base.BaseLLM.Config'>\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.base.query_pipeline.query.ChainableMixin:\n",
      " |\n",
      " |  as_query_component(self, partial: Optional[Dict[str, Any]] = None, **kwargs: Any) -> 'QueryComponent'\n",
      " |      Get query component.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.core.base.query_pipeline.query.ChainableMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.schema.BaseComponent:\n",
      " |\n",
      " |  __getstate__(self) -> Dict[str, Any]\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __setstate__(self, state: Dict[str, Any]) -> None\n",
      " |\n",
      " |  dict(self, **kwargs: Any) -> Dict[str, Any]\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |  json(self, **kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |\n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |\n",
      " |  to_dict(self, **kwargs: Any) -> Dict[str, Any]\n",
      " |\n",
      " |  to_json(self, **kwargs: Any) -> str\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from llama_index.core.schema.BaseComponent:\n",
      " |\n",
      " |  from_dict(data: Dict[str, Any], **kwargs: Any) -> Self\n",
      " |      # TODO: return type here not supported by current mypy version\n",
      " |\n",
      " |  from_json(data_str: str, **kwargs: Any) -> Self\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |\n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |\n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |\n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __get_validators__() -> 'CallableGenerator'\n",
      " |\n",
      " |  __try_update_forward_refs__(**localns: Any) -> None\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |\n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model'\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |\n",
      " |  from_orm(obj: Any) -> 'Model'\n",
      " |\n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model'\n",
      " |\n",
      " |  parse_obj(obj: Any) -> 'Model'\n",
      " |\n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model'\n",
      " |\n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny'\n",
      " |\n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str\n",
      " |\n",
      " |  update_forward_refs(**localns: Any) -> None\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |\n",
      " |  validate(value: Any) -> 'Model'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |\n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |\n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can use the `llama_index` module from Python's Hugging Face Transformers library to load a LLaMA model into GPU\n",
      "memory. Here's an example:\n",
      "```python\n",
      "import torch\n",
      "from transformers import LLamaIndex, AutoModelForCausalLM\n",
      "\n",
      "# Load the model and tokenizer\n",
      "model_name = \"llama-base-1.3b\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
      "\n",
      "# Move the model to GPU (assuming you have a CUDA-compatible NVIDIA GPU)\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "model.to(device)\n",
      "\n",
      "# Create an LLamaIndex instance\n",
      "index = LLamaIndex(model, tokenizer)\n",
      "\n",
      "# You can now use the index to generate text, etc.\n",
      "input_ids = ...  # your input IDs\n",
      "output = index.generate(input_ids)\n",
      "print(output)\n",
      "```\n",
      "In this example, we load the `llama-base-1.3b` model and tokenizer using the `AutoModelForCausalLM` and `AutoTokenizer`\n",
      "classes from the Transformers library. We then move the model to a CUDA-compatible GPU (or CPU if no GPU is available)\n",
      "using the `to()` method.\n",
      "\n",
      "Next, we create an instance of the `LLamaIndex` class, passing in the loaded model and tokenizer. The `LLamaIndex`\n",
      "instance will keep the model in memory on the specified device (GPU or CPU).\n",
      "\n",
      "From this point on, you can use the `index` object to generate text, answer questions, or perform other tasks that\n",
      "require the LLaMA model.\n",
      "\n",
      "Note that keeping a large model like LLaMA in GPU memory may consume significant resources. Be sure to check your\n",
      "system's available memory and adjust as needed.\n"
     ]
    }
   ],
   "source": [
    "print(fmt(str(llm_big_gpu.complete(\"Is there a way using ollama and python's llama_index to keep a model in the gpu memory?\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets push to see how big a model will run in the 3070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with that!\n",
      "\n",
      "Here's a simple yet elegant recipe for a classic cocktail:\n",
      "\n",
      "**Bee's Knees**\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 oz (60 ml) gin\n",
      "* 1 oz (30 ml) honey syrup (equal parts honey and water, dissolved)\n",
      "* 1/2 oz (15 ml) fresh lemon juice\n",
      "* Dash of sparkling water\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Fill a cocktail shaker with ice.\n",
      "2. Add the gin, honey syrup, and lemon juice.\n",
      "3. Shake vigorously for about 10-12 seconds to combine and chill the ingredients.\n",
      "4. Strain the mixture into a chilled coupe or cocktail glass.\n",
      "5. Top with a dash of sparkling water.\n",
      "\n",
      "Garnish with a lemon twist or wheel, if desired.\n",
      "\n",
      "**Why it's great:**\n",
      "\n",
      "* The honey syrup adds a touch of sweetness without overpowering the other flavors.\n",
      "* The lemon juice provides a nice acidity and brightness to balance out the drink.\n",
      "* Gin is a versatile spirit that pairs well with the floral notes from the honey.\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "* Adjust the amount of honey syrup to your taste, depending on how sweet you like your cocktails.\n",
      "* Substitute other spirits, such as vodka or rum, for a different flavor profile.\n",
      "* Add a splash of citrus-flavored liqueur, like Grand Marnier or Cointreau, for added depth and complexity.\n",
      "* Experiment with different garnishes, like a sprig of rosemary or a slice of orange, to find your favorite combination.\n",
      "\n",
      "I hope you enjoy this recipe! Do you have any specific preferences or flavor profiles in mind? I'd be happy to help you\n",
      "create a custom cocktail recipe.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(base_url=\"http://192.168.86.2:11433\", model=\"llama3\", request_timeout=360.0, keep_alive=-1)\n",
    "\n",
    "print(fmt(str(llm.complete(\"What is a good recipe for a cocktail?\", keep_alive=-1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets play around with some agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    [multiply_tool, add_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm_args={ \"keep_alive\": \"-1m\" },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The user wants to know the value of (121 + 2) * 5. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {'a': 123, 'b': 5}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 615\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The user observed that the result of (121 + 2) * 5 is indeed 615. I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: 615\n",
      "\u001b[0m615\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (121 + 2) * 5?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
